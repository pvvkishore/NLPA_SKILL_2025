{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP9j4fFsFl0Jd+78bhJLXWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvvkishore/NLPA_SKILL_2025/blob/main/Text_Classification_Pipeline_ANNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhFe3JDoBsyQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from collections import Counter, defaultdict\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class TextDataset:\n",
        "    \"\"\"\n",
        "    Create and manage a text classification dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.create_dataset()\n",
        "        self.display_dataset_characteristics()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        \"\"\"Create a 3-class sentiment classification dataset\"\"\"\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"STEP 1: CREATING TEXT CLASSIFICATION DATASET\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Create training data\n",
        "        self.train_data = [\n",
        "            # Positive sentiment\n",
        "            (\"I love this movie it's absolutely amazing\", \"positive\"),\n",
        "            (\"This is the best product I have ever bought\", \"positive\"),\n",
        "            (\"Fantastic experience wonderful service\", \"positive\"),\n",
        "            (\"Great quality excellent value for money\", \"positive\"),\n",
        "            (\"Perfect exactly what I needed highly recommend\", \"positive\"),\n",
        "            (\"Outstanding performance superb design\", \"positive\"),\n",
        "            (\"Brilliant idea love the innovation\", \"positive\"),\n",
        "            (\"Excellent customer service very helpful\", \"positive\"),\n",
        "            (\"Amazing results exceeded my expectations\", \"positive\"),\n",
        "            (\"Beautiful design high quality materials\", \"positive\"),\n",
        "            (\"Wonderful experience great staff\", \"positive\"),\n",
        "            (\"Perfect solution exactly what I wanted\", \"positive\"),\n",
        "\n",
        "            # Negative sentiment\n",
        "            (\"This movie is terrible waste of time\", \"negative\"),\n",
        "            (\"Worst product ever complete disappointment\", \"negative\"),\n",
        "            (\"Horrible service never buying again\", \"negative\"),\n",
        "            (\"Poor quality broke after one day\", \"negative\"),\n",
        "            (\"Terrible experience bad customer service\", \"negative\"),\n",
        "            (\"Awful design very uncomfortable\", \"negative\"),\n",
        "            (\"Disappointing results not worth the money\", \"negative\"),\n",
        "            (\"Bad quality cheap materials\", \"negative\"),\n",
        "            (\"Horrible experience rude staff\", \"negative\"),\n",
        "            (\"Terrible product poor performance\", \"negative\"),\n",
        "            (\"Worst service ever experienced\", \"negative\"),\n",
        "            (\"Complete waste of money very disappointed\", \"negative\"),\n",
        "\n",
        "            # Neutral sentiment\n",
        "            (\"The product is okay nothing special\", \"neutral\"),\n",
        "            (\"Average service acceptable quality\", \"neutral\"),\n",
        "            (\"It works fine basic functionality\", \"neutral\"),\n",
        "            (\"Standard product meets basic requirements\", \"neutral\"),\n",
        "            (\"Decent quality reasonable price\", \"neutral\"),\n",
        "            (\"Normal experience as expected\", \"neutral\"),\n",
        "            (\"Regular service standard quality\", \"neutral\"),\n",
        "            (\"It's fine nothing extraordinary\", \"neutral\"),\n",
        "            (\"Average performance acceptable results\", \"neutral\"),\n",
        "            (\"Standard design basic features\", \"neutral\"),\n",
        "            (\"Normal quality adequate for the price\", \"neutral\"),\n",
        "            (\"Acceptable product does the job\", \"neutral\"),\n",
        "        ]\n",
        "\n",
        "        # Create test data\n",
        "        self.test_data = [\n",
        "            (\"This is amazing I love it\", \"positive\"),\n",
        "            (\"Terrible quality very disappointed\", \"negative\"),\n",
        "            (\"It's okay nothing special\", \"neutral\"),\n",
        "            (\"Excellent product highly recommend\", \"positive\"),\n",
        "            (\"Worst experience ever\", \"negative\"),\n",
        "            (\"Standard quality acceptable\", \"neutral\"),\n",
        "        ]\n",
        "\n",
        "        # Create inference data (unlabeled)\n",
        "        self.inference_data = [\n",
        "            \"This product is fantastic\",\n",
        "            \"Really bad experience\",\n",
        "            \"It's just average\",\n",
        "            \"Outstanding quality\",\n",
        "            \"Poor service\",\n",
        "            \"Normal product\"\n",
        "        ]\n",
        "\n",
        "        print(f\"‚úÖ Dataset created successfully!\")\n",
        "        print(f\"   Training samples: {len(self.train_data)}\")\n",
        "        print(f\"   Test samples: {len(self.test_data)}\")\n",
        "        print(f\"   Inference samples: {len(self.inference_data)}\")\n",
        "\n",
        "    def display_dataset_characteristics(self):\n",
        "        \"\"\"Display comprehensive dataset analysis\"\"\"\n",
        "\n",
        "        print(f\"\\nüìä DATASET CHARACTERISTICS ANALYSIS\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # Extract texts and labels\n",
        "        train_texts, train_labels = zip(*self.train_data)\n",
        "        test_texts, test_labels = zip(*self.test_data)\n",
        "\n",
        "        # Class distribution\n",
        "        train_label_counts = Counter(train_labels)\n",
        "        test_label_counts = Counter(test_labels)\n",
        "\n",
        "        print(f\"üè∑Ô∏è  CLASS DISTRIBUTION:\")\n",
        "        print(f\"   Training set:\")\n",
        "        for label, count in train_label_counts.items():\n",
        "            percentage = (count / len(train_labels)) * 100\n",
        "            print(f\"     {label}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        print(f\"   Test set:\")\n",
        "        for label, count in test_label_counts.items():\n",
        "            percentage = (count / len(test_labels)) * 100\n",
        "            print(f\"     {label}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        # Text length analysis\n",
        "        train_lengths = [len(text.split()) for text in train_texts]\n",
        "        test_lengths = [len(text.split()) for text in test_texts]\n",
        "\n",
        "        print(f\"\\nüìè TEXT LENGTH ANALYSIS:\")\n",
        "        print(f\"   Training set - Min: {min(train_lengths)}, Max: {max(train_lengths)}, Avg: {np.mean(train_lengths):.1f}\")\n",
        "        print(f\"   Test set - Min: {min(test_lengths)}, Max: {max(test_lengths)}, Avg: {np.mean(test_lengths):.1f}\")\n",
        "\n",
        "        # Vocabulary analysis\n",
        "        all_words = []\n",
        "        for text in train_texts:\n",
        "            all_words.extend(text.lower().split())\n",
        "\n",
        "        vocab_size = len(set(all_words))\n",
        "        most_common = Counter(all_words).most_common(10)\n",
        "\n",
        "        print(f\"\\nüìö VOCABULARY ANALYSIS:\")\n",
        "        print(f\"   Total words: {len(all_words)}\")\n",
        "        print(f\"   Unique words: {vocab_size}\")\n",
        "        print(f\"   Most common words: {most_common[:5]}\")\n",
        "\n",
        "        # Sample data display\n",
        "        print(f\"\\nüìã SAMPLE DATA STRUCTURE:\")\n",
        "        print(f\"   Training samples:\")\n",
        "        for i, (text, label) in enumerate(self.train_data[:3]):\n",
        "            print(f\"     {i+1}. Text: '{text}'\")\n",
        "            print(f\"        Label: '{label}'\")\n",
        "\n",
        "        return {\n",
        "            'train_samples': len(self.train_data),\n",
        "            'test_samples': len(self.test_data),\n",
        "            'num_classes': len(train_label_counts),\n",
        "            'vocab_size': vocab_size,\n",
        "            'avg_length': np.mean(train_lengths),\n",
        "            'class_distribution': dict(train_label_counts)\n",
        "        }\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"\n",
        "    Handle text preprocessing and vocabulary building\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def build_vocabulary(self, texts: List[str]) -> None:\n",
        "        \"\"\"Build vocabulary from training texts\"\"\"\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"STEP 2: TEXT PREPROCESSING & VOCABULARY BUILDING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Collect all words\n",
        "        all_words = []\n",
        "        for text in texts:\n",
        "            # Basic preprocessing\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "            words = text.split()\n",
        "            all_words.extend(words)\n",
        "\n",
        "        # Create vocabulary\n",
        "        word_counts = Counter(all_words)\n",
        "\n",
        "        # Add special tokens\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "\n",
        "        # Add words to vocabulary\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= 1:  # Keep all words for this small dataset\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "        print(f\"‚úÖ Vocabulary built successfully!\")\n",
        "        print(f\"   Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"   Most common words: {word_counts.most_common(10)}\")\n",
        "\n",
        "        # Display word2idx mapping (first 15 entries)\n",
        "        print(f\"\\nüóÇÔ∏è  WORD2IDX MAPPING (first 15 entries):\")\n",
        "        for i, (word, idx) in enumerate(list(self.word2idx.items())[:15]):\n",
        "            print(f\"   '{word}' ‚Üí {idx}\")\n",
        "\n",
        "        # Display idx2word mapping (first 15 entries)\n",
        "        print(f\"\\nüóÇÔ∏è  IDX2WORD MAPPING (first 15 entries):\")\n",
        "        for i, (idx, word) in enumerate(list(self.idx2word.items())[:15]):\n",
        "            print(f\"   {idx} ‚Üí '{word}'\")\n",
        "\n",
        "    def text_to_sequence(self, text: str, max_length: int = 10) -> List[int]:\n",
        "        \"\"\"Convert text to sequence of indices\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        words = text.split()\n",
        "\n",
        "        # Convert words to indices\n",
        "        sequence = []\n",
        "        for word in words:\n",
        "            if word in self.word2idx:\n",
        "                sequence.append(self.word2idx[word])\n",
        "            else:\n",
        "                sequence.append(self.word2idx['<UNK>'])\n",
        "\n",
        "        # Pad or truncate to max_length\n",
        "        if len(sequence) < max_length:\n",
        "            sequence.extend([self.word2idx['<PAD>']] * (max_length - len(sequence)))\n",
        "        else:\n",
        "            sequence = sequence[:max_length]\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def sequence_to_text(self, sequence: List[int]) -> str:\n",
        "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
        "        words = []\n",
        "        for idx in sequence:\n",
        "            if idx in self.idx2word and self.idx2word[idx] != '<PAD>':\n",
        "                words.append(self.idx2word[idx])\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def process_dataset(self, data: List[Tuple[str, str]], max_length: int = 10):\n",
        "        \"\"\"Process entire dataset\"\"\"\n",
        "\n",
        "        print(f\"\\nüìù PROCESSING DATASET:\")\n",
        "        print(f\"   Max sequence length: {max_length}\")\n",
        "\n",
        "        # Create label mapping\n",
        "        labels = [label for _, label in data]\n",
        "        unique_labels = sorted(list(set(labels)))\n",
        "        label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        idx2label = {idx: label for label, idx in label2idx.items()}\n",
        "\n",
        "        print(f\"   Label mapping: {label2idx}\")\n",
        "\n",
        "        # Process texts and labels\n",
        "        sequences = []\n",
        "        label_indices = []\n",
        "\n",
        "        print(f\"\\nüîÑ CONVERSION EXAMPLES:\")\n",
        "        for i, (text, label) in enumerate(data[:3]):\n",
        "            sequence = self.text_to_sequence(text, max_length)\n",
        "            label_idx = label2idx[label]\n",
        "\n",
        "            sequences.append(sequence)\n",
        "            label_indices.append(label_idx)\n",
        "\n",
        "            print(f\"   Example {i+1}:\")\n",
        "            print(f\"     Original: '{text}' ‚Üí {label}\")\n",
        "            print(f\"     Sequence: {sequence}\")\n",
        "            print(f\"     Label idx: {label_idx}\")\n",
        "            print(f\"     Back to text: '{self.sequence_to_text(sequence)}'\")\n",
        "\n",
        "        # Process all data\n",
        "        for text, label in data[3:]:\n",
        "            sequence = self.text_to_sequence(text, max_length)\n",
        "            label_idx = label2idx[label]\n",
        "            sequences.append(sequence)\n",
        "            label_indices.append(label_idx)\n",
        "\n",
        "        return np.array(sequences), np.array(label_indices), label2idx, idx2label\n",
        "\n",
        "class TextClassifierDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for text classification\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.LongTensor(sequences)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "class TextClassifierNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Neural Network for Text Classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super(TextClassifierNN, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Neural network layers\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è  NEURAL NETWORK ARCHITECTURE:\")\n",
        "        print(f\"   Embedding Layer: {vocab_size} ‚Üí {embedding_dim}\")\n",
        "        print(f\"   Hidden Layer 1: {embedding_dim} ‚Üí {hidden_dim}\")\n",
        "        print(f\"   Hidden Layer 2: {hidden_dim} ‚Üí {hidden_dim // 2}\")\n",
        "        print(f\"   Output Layer: {hidden_dim // 2} ‚Üí {num_classes}\")\n",
        "        print(f\"   Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Average pooling over sequence length\n",
        "        pooled = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Neural network\n",
        "        x = F.relu(self.fc1(pooled))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_embeddings(self, sequences):\n",
        "        \"\"\"Get embedding vectors for sequences\"\"\"\n",
        "        with torch.no_grad():\n",
        "            embedded = self.embedding(sequences)\n",
        "            pooled = torch.mean(embedded, dim=1)\n",
        "            return pooled.numpy()\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"\n",
        "    Handle model training and evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, train_loader, test_loader, num_classes):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.test_accuracies = []\n",
        "\n",
        "        # Store weights at different stages\n",
        "        self.initial_weights = {}\n",
        "        self.middle_weights = {}\n",
        "        self.final_weights = {}\n",
        "\n",
        "    def save_weights(self, stage: str):\n",
        "        \"\"\"Save model weights at different training stages\"\"\"\n",
        "        weights = {}\n",
        "        for name, param in self.model.named_parameters():\n",
        "            weights[name] = param.data.clone()\n",
        "\n",
        "        if stage == 'initial':\n",
        "            self.initial_weights = weights\n",
        "        elif stage == 'middle':\n",
        "            self.middle_weights = weights\n",
        "        elif stage == 'final':\n",
        "            self.final_weights = weights\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        self.train_losses.append(avg_loss)\n",
        "        self.train_accuracies.append(accuracy)\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate model on test set\"\"\"\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.test_loader:\n",
        "                output = self.model(data)\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        self.test_accuracies.append(accuracy)\n",
        "\n",
        "        return accuracy, all_preds, all_targets\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        \"\"\"Complete training process\"\"\"\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"STEP 4: NEURAL NETWORK TRAINING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Save initial weights\n",
        "        self.save_weights('initial')\n",
        "\n",
        "        print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"   Optimizer: Adam (lr=0.001)\")\n",
        "        print(f\"   Loss function: CrossEntropyLoss\")\n",
        "        print(f\"   Batch size: {self.train_loader.batch_size}\")\n",
        "\n",
        "        print(f\"\\nüìä TRAINING PROGRESS:\")\n",
        "        print(f\"{'Epoch':<6} {'Train Loss':<12} {'Train Acc':<12} {'Test Acc':<12}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc = self.train_epoch(epoch)\n",
        "            test_acc, _, _ = self.evaluate()\n",
        "\n",
        "            # Save middle weights at halfway point\n",
        "            if epoch == num_epochs // 2:\n",
        "                self.save_weights('middle')\n",
        "\n",
        "            print(f\"{epoch+1:<6} {train_loss:<12.4f} {train_acc:<12.1f}% {test_acc:<12.1f}%\")\n",
        "\n",
        "        # Save final weights\n",
        "        self.save_weights('final')\n",
        "\n",
        "        print(f\"\\n‚úÖ Training completed!\")\n",
        "        print(f\"   Final train accuracy: {self.train_accuracies[-1]:.1f}%\")\n",
        "        print(f\"   Final test accuracy: {self.test_accuracies[-1]:.1f}%\")\n",
        "\n",
        "    def display_weight_analysis(self):\n",
        "        \"\"\"Display weight changes during training\"\"\"\n",
        "\n",
        "        print(f\"\\nüîç WEIGHT ANALYSIS:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Analyze embedding weights\n",
        "        emb_initial = self.initial_weights['embedding.weight']\n",
        "        emb_middle = self.middle_weights['embedding.weight']\n",
        "        emb_final = self.final_weights['embedding.weight']\n",
        "\n",
        "        print(f\"üìä Embedding Layer Weight Changes:\")\n",
        "        print(f\"   Initial weights (first 5 words, first 3 dims):\")\n",
        "        print(f\"   {emb_initial[:5, :3].numpy()}\")\n",
        "        print(f\"   Final weights (first 5 words, first 3 dims):\")\n",
        "        print(f\"   {emb_final[:5, :3].numpy()}\")\n",
        "\n",
        "        # Calculate weight change magnitude\n",
        "        weight_change = torch.norm(emb_final - emb_initial).item()\n",
        "        print(f\"   Total embedding weight change magnitude: {weight_change:.4f}\")\n",
        "\n",
        "        # Analyze first linear layer\n",
        "        fc1_initial = self.initial_weights['fc1.weight']\n",
        "        fc1_final = self.final_weights['fc1.weight']\n",
        "        fc1_change = torch.norm(fc1_final - fc1_initial).item()\n",
        "\n",
        "        print(f\"\\nüìä First Linear Layer Weight Changes:\")\n",
        "        print(f\"   Weight change magnitude: {fc1_change:.4f}\")\n",
        "        print(f\"   Initial weights (first 3x3):\")\n",
        "        print(f\"   {fc1_initial[:3, :3].numpy()}\")\n",
        "        print(f\"   Final weights (first 3x3):\")\n",
        "        print(f\"   {fc1_final[:3, :3].numpy()}\")\n",
        "\n",
        "def display_embeddings(model, preprocessor, sample_texts, idx2label):\n",
        "    \"\"\"Display embedding vectors for sample texts\"\"\"\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 5: EMBEDDING ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"üî§ EMBEDDING VECTORS FOR SAMPLE TEXTS:\")\n",
        "\n",
        "    for i, text in enumerate(sample_texts[:3]):\n",
        "        sequence = preprocessor.text_to_sequence(text)\n",
        "        sequence_tensor = torch.LongTensor([sequence])\n",
        "\n",
        "        # Get embedding\n",
        "        embeddings = model.get_embeddings(sequence_tensor)\n",
        "        embedding_vector = embeddings[0]\n",
        "\n",
        "        print(f\"\\n   Text {i+1}: '{text}'\")\n",
        "        print(f\"   Sequence: {sequence}\")\n",
        "        print(f\"   Embedding vector (first 10 dims): {embedding_vector[:10]}\")\n",
        "        print(f\"   Embedding shape: {embedding_vector.shape}\")\n",
        "        print(f\"   Embedding magnitude: {np.linalg.norm(embedding_vector):.4f}\")\n",
        "\n",
        "def plot_training_metrics(trainer):\n",
        "    \"\"\"Plot training metrics\"\"\"\n",
        "\n",
        "    print(f\"\\nüìà PLOTTING TRAINING METRICS...\")\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training loss\n",
        "    ax1.plot(trainer.train_losses, 'b-', linewidth=2)\n",
        "    ax1.set_title('Training Loss Over Time')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Training accuracy\n",
        "    ax2.plot(trainer.train_accuracies, 'g-', linewidth=2, label='Train')\n",
        "    ax2.plot(trainer.test_accuracies, 'r-', linewidth=2, label='Test')\n",
        "    ax2.set_title('Accuracy Over Time')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Weight change visualization\n",
        "    epochs = range(len(trainer.train_losses))\n",
        "    ax3.bar(epochs, trainer.train_losses, alpha=0.7, color='skyblue')\n",
        "    ax3.set_title('Loss Distribution by Epoch')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Loss')\n",
        "\n",
        "    # Performance summary\n",
        "    ax4.text(0.1, 0.8, f\"Final Training Accuracy: {trainer.train_accuracies[-1]:.1f}%\",\n",
        "             fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.text(0.1, 0.6, f\"Final Test Accuracy: {trainer.test_accuracies[-1]:.1f}%\",\n",
        "             fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.text(0.1, 0.4, f\"Best Test Accuracy: {max(trainer.test_accuracies):.1f}%\",\n",
        "             fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.text(0.1, 0.2, f\"Total Epochs: {len(trainer.train_losses)}\",\n",
        "             fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.set_title('Training Summary')\n",
        "    ax4.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def test_model_inference(model, preprocessor, test_data, inference_data, idx2label):\n",
        "    \"\"\"Test model on new data and show inference process\"\"\"\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 6: MODEL TESTING & INFERENCE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Test on labeled test data\n",
        "    print(f\"üß™ TESTING ON LABELED DATA:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for i, (text, true_label) in enumerate(test_data):\n",
        "        sequence = preprocessor.text_to_sequence(text)\n",
        "        sequence_tensor = torch.LongTensor([sequence])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sequence_tensor)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            predicted_idx = output.argmax(dim=1).item()\n",
        "            predicted_label = idx2label[predicted_idx]\n",
        "            confidence = probabilities[0][predicted_idx].item()\n",
        "\n",
        "        all_predictions.append(predicted_idx)\n",
        "        all_true_labels.append(list(idx2label.values()).index(true_label))\n",
        "\n",
        "        print(f\"   Test {i+1}:\")\n",
        "        print(f\"     Text: '{text}'\")\n",
        "        print(f\"     True label: {true_label}\")\n",
        "        print(f\"     Predicted: {predicted_label}\")\n",
        "        print(f\"     Confidence: {confidence:.3f}\")\n",
        "        print(f\"     Probabilities: {dict(zip(idx2label.values(), probabilities[0].numpy()))}\")\n",
        "        print()\n",
        "\n",
        "    # Calculate test metrics\n",
        "    test_accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "    print(f\"üìä TEST RESULTS:\")\n",
        "    print(f\"   Test Accuracy: {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\nüìã DETAILED CLASSIFICATION REPORT:\")\n",
        "    report = classification_report(all_true_labels, all_predictions,\n",
        "                                 target_names=list(idx2label.values()),\n",
        "                                 zero_division=0)\n",
        "    print(report)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "    print(f\"üî¢ CONFUSION MATRIX:\")\n",
        "    print(f\"   {cm}\")\n",
        "\n",
        "    # Test on inference data (unlabeled)\n",
        "    print(f\"\\nüîÆ INFERENCE ON NEW DATA:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for i, text in enumerate(inference_data):\n",
        "        sequence = preprocessor.text_to_sequence(text)\n",
        "        sequence_tensor = torch.LongTensor([sequence])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sequence_tensor)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            predicted_idx = output.argmax(dim=1).item()\n",
        "            predicted_label = idx2label[predicted_idx]\n",
        "            confidence = probabilities[0][predicted_idx].item()\n",
        "\n",
        "        print(f\"   Inference {i+1}:\")\n",
        "        print(f\"     Text: '{text}'\")\n",
        "        print(f\"     Predicted: {predicted_label}\")\n",
        "        print(f\"     Confidence: {confidence:.3f}\")\n",
        "        print(f\"     All probabilities:\")\n",
        "        for label, prob in zip(idx2label.values(), probabilities[0].numpy()):\n",
        "            print(f\"       {label}: {prob:.3f}\")\n",
        "        print()\n",
        "\n",
        "def save_model_and_artifacts(model, preprocessor, idx2label, trainer):\n",
        "    \"\"\"Save trained model and related artifacts\"\"\"\n",
        "\n",
        "    print(f\"\\nüíæ SAVING MODEL AND ARTIFACTS:\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'text_classifier_model.pth')\n",
        "    print(f\"   ‚úÖ Model saved: text_classifier_model.pth\")\n",
        "\n",
        "    # Save preprocessor\n",
        "    with open('preprocessor.pkl', 'wb') as f:\n",
        "        pickle.dump(preprocessor, f)\n",
        "    print(f\"   ‚úÖ Preprocessor saved: preprocessor.pkl\")\n",
        "\n",
        "    # Save label mappings\n",
        "    with open('label_mappings.json', 'w') as f:\n",
        "        json.dump({'idx2label': idx2label}, f, indent=2)\n",
        "    print(f\"   ‚úÖ Label mappings saved: label_mappings.json\")\n",
        "\n",
        "    # Save training history\n",
        "    training_history = {\n",
        "        'train_losses': trainer.train_losses,\n",
        "        'train_accuracies': trainer.train_accuracies,\n",
        "        'test_accuracies': trainer.test_accuracies\n",
        "    }\n",
        "    with open('training_history.json', 'w') as f:\n",
        "        json.dump(training_history, f, indent=2)\n",
        "    print(f\"   ‚úÖ Training history saved: training_history.json\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete text classification pipeline\"\"\"\n",
        "\n",
        "    print(\"üöÄ COMPLETE TEXT CLASSIFICATION NEURAL NETWORK TUTORIAL\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Create dataset\n",
        "    dataset = TextDataset()\n",
        "\n",
        "    # Step 2: Build vocabulary and preprocess\n",
        "    preprocessor = TextPreprocessor()\n",
        "    train_texts = [text for text, _ in dataset.train_data]\n",
        "    preprocessor.build_vocabulary(train_texts)\n",
        "\n",
        "    # Process datasets\n",
        "    max_length = 10\n",
        "    train_sequences, train_labels, label2idx, idx2label = preprocessor.process_dataset(\n",
        "        dataset.train_data, max_length)\n",
        "    test_sequences, test_labels, _, _ = preprocessor.process_dataset(\n",
        "        dataset.test_data, max_length)\n",
        "\n",
        "    print(f\"\\nüîÑ DATASET PROCESSING COMPLETE:\")\n",
        "    print(f\"   Training sequences shape: {train_sequences.shape}\")\n",
        "    print(f\"   Training labels shape: {train_labels.shape}\")\n",
        "    print(f\"   Test sequences shape: {test_sequences.shape}\")\n",
        "    print(f\"   Test labels shape: {test_labels.shape}\")\n",
        "\n",
        "    # Step 3: Create data loaders\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 3: CREATING DATA LOADERS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    train_dataset = TextClassifierDataset(train_sequences, train_labels)\n",
        "    test_dataset = TextClassifierDataset(test_sequences, test_labels)\n",
        "\n",
        "    batch_size = 8\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"‚úÖ Data loaders created:\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Test batches: {len(test_loader)}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "    # Display sample batch\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    sample_sequences, sample_labels = sample_batch\n",
        "    print(f\"\\nüì¶ SAMPLE BATCH:\")\n",
        "    print(f\"   Batch sequences shape: {sample_sequences.shape}\")\n",
        "    print(f\"   Batch labels shape: {sample_labels.shape}\")\n",
        "    print(f\"   Sample sequence: {sample_sequences[0].numpy()}\")\n",
        "    print(f\"   Sample label: {sample_labels[0].item()} ({idx2label[sample_labels[0].item()]})\")\n",
        "\n",
        "    # Initialize model\n",
        "    vocab_size = preprocessor.vocab_size\n",
        "    embedding_dim = 16\n",
        "    hidden_dim = 32\n",
        "    num_classes = len(label2idx)\n",
        "\n",
        "    model = TextClassifierNN(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "    # Display model architecture details\n",
        "    print(f\"\\nüèóÔ∏è  MODEL ARCHITECTURE DETAILS:\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"   Total parameters: {total_params:,}\")\n",
        "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Show layer details\n",
        "    for name, module in model.named_modules():\n",
        "        if len(list(module.children())) == 0:  # Leaf modules only\n",
        "            params = sum(p.numel() for p in module.parameters())\n",
        "            print(f\"   {name}: {params:,} parameters\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = ModelTrainer(model, train_loader, test_loader, num_classes)\n",
        "\n",
        "    # Train model\n",
        "    num_epochs = 20\n",
        "    trainer.train(num_epochs)\n",
        "\n",
        "    # Display weight analysis\n",
        "    trainer.display_weight_analysis()\n",
        "\n",
        "    # Display embeddings\n",
        "    sample_texts = [text for text, _ in dataset.train_data[:3]]\n",
        "    display_embeddings(model, preprocessor, sample_texts, idx2label)\n",
        "\n",
        "    # Plot training metrics\n",
        "    plot_training_metrics(trainer)\n",
        "\n",
        "    # Test model\n",
        "    test_model_inference(model, preprocessor, dataset.test_data,\n",
        "                        dataset.inference_data, idx2label)\n",
        "\n",
        "    # Save everything\n",
        "    save_model_and_artifacts(model, preprocessor, idx2label, trainer)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"üéâ TUTORIAL COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"‚úÖ Successfully trained a text classification neural network!\")\n",
        "    print(f\"‚úÖ Model achieved {trainer.test_accuracies[-1]:.1f}% test accuracy\")\n",
        "    print(f\"‚úÖ All artifacts saved for future use\")\n",
        "    print(f\"‚úÖ Visualizations generated\")\n",
        "\n",
        "    print(f\"\\nüìö WHAT THE MODEL LEARNED:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üéØ The neural network learned to:\")\n",
        "    print(f\"   ‚Ä¢ Map words to meaningful vector representations\")\n",
        "    print(f\"   ‚Ä¢ Distinguish between positive, negative, and neutral sentiments\")\n",
        "    print(f\"   ‚Ä¢ Generalize from training examples to new text\")\n",
        "    print(f\"   ‚Ä¢ Assign confidence scores to predictions\")\n",
        "\n",
        "    print(f\"\\nüîç KEY INSIGHTS:\")\n",
        "    print(f\"   ‚Ä¢ Embedding vectors capture semantic meaning\")\n",
        "    print(f\"   ‚Ä¢ Training gradually adjusts weights to improve accuracy\")\n",
        "    print(f\"   ‚Ä¢ Model confidence indicates prediction reliability\")\n",
        "    print(f\"   ‚Ä¢ Performance metrics help evaluate model quality\")\n",
        "\n",
        "    print(f\"\\nüöÄ NEXT STEPS:\")\n",
        "    print(f\"   ‚Ä¢ Try larger datasets for better performance\")\n",
        "    print(f\"   ‚Ä¢ Experiment with different architectures (LSTM, Transformer)\")\n",
        "    print(f\"   ‚Ä¢ Use pre-trained embeddings (Word2Vec, GloVe)\")\n",
        "    print(f\"   ‚Ä¢ Apply to real-world text classification problems\")\n",
        "\n",
        "def load_and_use_saved_model():\n",
        "    \"\"\"Demonstrate how to load and use the saved model\"\"\"\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"BONUS: LOADING AND USING SAVED MODEL\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Load preprocessor\n",
        "        with open('preprocessor.pkl', 'rb') as f:\n",
        "            preprocessor = pickle.load(f)\n",
        "\n",
        "        # Load label mappings\n",
        "        with open('label_mappings.json', 'r') as f:\n",
        "            mappings = json.load(f)\n",
        "            idx2label = {int(k): v for k, v in mappings['idx2label'].items()}\n",
        "\n",
        "        # Initialize model with same architecture\n",
        "        vocab_size = preprocessor.vocab_size\n",
        "        embedding_dim = 16\n",
        "        hidden_dim = 32\n",
        "        num_classes = len(idx2label)\n",
        "\n",
        "        model = TextClassifierNN(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "        # Load trained weights\n",
        "        model.load_state_dict(torch.load('text_classifier_model.pth'))\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "        # Test on new examples\n",
        "        new_texts = [\n",
        "            \"This is absolutely fantastic!\",\n",
        "            \"Really disappointing experience\",\n",
        "            \"It's okay, nothing special\"\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nüîÆ PREDICTIONS ON NEW TEXT:\")\n",
        "        for text in new_texts:\n",
        "            sequence = preprocessor.text_to_sequence(text)\n",
        "            sequence_tensor = torch.LongTensor([sequence])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(sequence_tensor)\n",
        "                probabilities = F.softmax(output, dim=1)\n",
        "                predicted_idx = output.argmax(dim=1).item()\n",
        "                predicted_label = idx2label[predicted_idx]\n",
        "                confidence = probabilities[0][predicted_idx].item()\n",
        "\n",
        "            print(f\"   Text: '{text}'\")\n",
        "            print(f\"   Prediction: {predicted_label} ({confidence:.3f})\")\n",
        "            print()\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå Could not load saved model: {e}\")\n",
        "        print(\"   Please run the main training pipeline first.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete tutorial\n",
        "    main()\n",
        "\n",
        "    # Demonstrate model loading\n",
        "    load_and_use_saved_model()"
      ]
    }
  ]
}