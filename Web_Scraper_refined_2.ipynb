{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7312bc6d-e702-4cdd-9f68-0c07ca4f5c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Selenium WebDriver initialized successfully\n",
      "INFO:__main__:Starting comprehensive KL University scraping...\n",
      "C:\\Users\\Dr.PVVK\\AppData\\Local\\Temp\\ipykernel_21828\\3249449777.py:339: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  address_elem = soup.find(text=re.compile('address|location|campus', re.I))\n",
      "INFO:__main__:Scraping main page notifications...\n",
      "INFO:__main__:Found 29 notifications\n",
      "INFO:__main__:Scraping departments...\n",
      "ERROR:__main__:Error fetching https://www.kluniversity.in/cse/ with requests: 404 Client Error: Not Found for url: https://www.kluniversity.in/cse/\n",
      "ERROR:__main__:Error fetching https://www.kluniversity.in/mech/ with requests: 404 Client Error: Not Found for url: https://www.kluniversity.in/mech/\n",
      "ERROR:__main__:Error fetching https://www.kluniversity.in/civil/ with requests: 404 Client Error: Not Found for url: https://www.kluniversity.in/civil/\n",
      "INFO:__main__:Found 3 departments\n",
      "INFO:__main__:Scraping courses and programs...\n",
      "INFO:__main__:Found 2 courses\n",
      "INFO:__main__:Scraping faculty information...\n",
      "C:\\Users\\Dr.PVVK\\AppData\\Local\\Temp\\ipykernel_21828\\3249449777.py:273: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  faculty_elements = soup.find_all(['div', 'td', 'span'], text=re.compile(r'Dr\\.|Prof\\.|Mr\\.|Ms\\.'))\n",
      "INFO:__main__:Found 0 faculty members\n",
      "INFO:__main__:Scraping achievements...\n",
      "C:\\Users\\Dr.PVVK\\AppData\\Local\\Temp\\ipykernel_21828\\3249449777.py:307: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  elements = soup.find_all(text=re.compile(keyword, re.I))\n",
      "INFO:__main__:Found 31 achievements\n",
      "INFO:__main__:Scraping completed successfully!\n",
      "INFO:__main__:Data saved to klu_scraped_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè´ KL University Scraping Summary:\n",
      "üì¢ Notifications: 20\n",
      "üè¢ Departments: 3\n",
      "üìö Courses: 2\n",
      "üë®‚Äçüè´ Faculty: 0\n",
      "üèÜ Achievements: 15\n",
      "üìû Contact Info: {'phones': ['1752835982196', '504496530959', '8832122115', '9849519527', '444468980536', '184624508233022', '+917997998383', '7815901716', '1752835982189', '1752835982242', '+917997995727', '1752835982249', '1752835981569', '1316721752', '917815926834', '154614221219641', '1752835981549', '1752835982'], 'address': '.trigger_popup {\\n                    transform: rotate(90deg) !important;\\n                    position: fixed; \\n                    top: 39%; \\n                   right:-46px;\\n                    z-index: 999;\\n                    cursor: pointer;\\n                    background-color: #b8292f;\\n                    border-color: #b8292f;\\n                    border-radius: 5px;\\n                    border-bottom-right-radius: 0;\\n                    border-bottom-left-radius: 0; \\n                    padding: 10px 12px;\\n                    font-size: 18px;\\n                    color: #fff;\\n                    line-height: 1.33;         \\n                    /* visibility: hidden;          */\\n                }\\n.trigger_popup:hover {\\n                    background-color: #d63232;\\n                    border-color: #d63232;\\n                }\\n\\n/* The Modal (background) */\\n.modal {\\n  display: none; /* Hidden by default */\\n  position: fixed; /* Stay in place */\\n  z-index: 9999; /* Sit on top */\\n  padding-top: 35px; /* Location of the box */\\n  left: 0;\\n  top: 0;\\n  width: 100%; /* Full width */\\n  height: 100%; /* Full height */\\n  overflow: auto; /* Enable scroll if needed */\\n  background-color: rgb(0,0,0); /* Fallback color */\\n  background-color: rgba(0,0,0,0.4); /* Black w/ opacity */\\n}\\n\\n/* Modal Content */\\n.modal-content {\\n  background-color: transparent;\\n  margin: auto;\\n  padding: 0;\\n  border: 0px solid #888;\\n  max-width: 390px;\\n  position: relative;\\n}\\n\\n/* The Close Button */\\n.close {\\n    color: #c1c1c1;\\n    float: right;\\n    font-size: 30px;\\n    font-weight: bold;\\n    position: absolute;\\n    right: 15px;\\n    z-index: 9999;\\n    top: 2px;\\n}\\n\\n.close:hover,\\n.close:focus {\\n  color: #797878;\\n  text-decoration: none;\\n  cursor: pointer;\\n}\\n.head_text {\\n    background-color: #dd3333;\\n    color: #fff;\\n    text-align: center;\\n    padding: 7px;\\n    font-size: 20px;\\n    border-top-left-radius: 35px;\\n}\\n\\n@media (max-width:768px) {\\n    .trigger_popup {\\n        transform: rotate(0);\\n        bottom: 0;\\n        top: 90%;\\n        right: unset;\\n        margin: 0;\\n        left: 15%;\\n        font-size: 14px;\\n        padding: 6px 10px;\\n    }\\n    .popupCloseButton {\\n        top: -10px;\\n        right: -2px;\\n    }\\n}'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "KL University Optimized Scraper\n",
    "Specifically designed for https://www.kluniversity.in/\n",
    "Handles ASP.NET postbacks, JavaScript content, and dynamic loading\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class KLUData:\n",
    "    \"\"\"Data structure for KL University information\"\"\"\n",
    "    notifications: List[Dict]\n",
    "    departments: List[Dict]\n",
    "    courses: List[Dict]\n",
    "    faculty: List[Dict]\n",
    "    news: List[Dict]\n",
    "    achievements: List[Dict]\n",
    "    contact_info: Dict\n",
    "    scraped_at: str\n",
    "\n",
    "class KLUniversityScraper:\n",
    "    \"\"\"\n",
    "    Specialized scraper for KL University website\n",
    "    Handles ASP.NET specific challenges:\n",
    "    - ViewState management\n",
    "    - PostBack events\n",
    "    - Dynamic content loading\n",
    "    - JavaScript-dependent sections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 3.0):\n",
    "        self.base_url = \"https://www.kluniversity.in\"\n",
    "        self.delay = delay\n",
    "        \n",
    "        # Setup session for requests\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # Setup Selenium\n",
    "        self.setup_selenium()\n",
    "    \n",
    "    def setup_selenium(self):\n",
    "        \"\"\"Configure Selenium WebDriver for KLU website\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--window-size=1920,1080')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            logger.info(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Selenium: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_page_with_selenium(self, url: str, wait_for_element: str = \"body\") -> BeautifulSoup:\n",
    "        \"\"\"Get page content using Selenium with proper wait\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for specific element to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, wait_for_element))\n",
    "            )\n",
    "            \n",
    "            # Additional wait for dynamic content\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            html = self.driver.page_source\n",
    "            return BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"Timeout waiting for {url} to load\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_page_with_requests(self, url: str) -> BeautifulSoup:\n",
    "        \"\"\"Get page content using requests (for static content)\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching {url} with requests: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_main_page_notifications(self) -> List[Dict]:\n",
    "        \"\"\"Scrape notifications from main page\"\"\"\n",
    "        logger.info(\"Scraping main page notifications...\")\n",
    "        \n",
    "        soup = self.get_page_with_selenium(self.base_url)\n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        notifications = []\n",
    "        \n",
    "        # Look for notification links (based on the content structure observed)\n",
    "        notification_patterns = [\n",
    "            'a[href*=\"pdf\"]',\n",
    "            'a[href*=\"notification\"]',\n",
    "            'a[href*=\"admit\"]',\n",
    "            'a[href*=\"admission\"]'\n",
    "        ]\n",
    "        \n",
    "        for pattern in notification_patterns:\n",
    "            elements = soup.select(pattern)\n",
    "            for elem in elements:\n",
    "                text = elem.get_text(strip=True)\n",
    "                href = elem.get('href')\n",
    "                \n",
    "                if href and text and len(text) > 10:\n",
    "                    notifications.append({\n",
    "                        'title': text,\n",
    "                        'url': urljoin(self.base_url, href),\n",
    "                        'type': 'notification',\n",
    "                        'date': self.extract_date_from_text(text)\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_notifications = []\n",
    "        for notif in notifications:\n",
    "            if notif['title'] not in seen:\n",
    "                seen.add(notif['title'])\n",
    "                unique_notifications.append(notif)\n",
    "        \n",
    "        logger.info(f\"Found {len(unique_notifications)} notifications\")\n",
    "        return unique_notifications[:20]  # Limit to recent 20\n",
    "    \n",
    "    def scrape_departments(self) -> List[Dict]:\n",
    "        \"\"\"Scrape department information\"\"\"\n",
    "        logger.info(\"Scraping departments...\")\n",
    "        \n",
    "        departments = []\n",
    "        \n",
    "        # Common department pages to check\n",
    "        dept_urls = [\n",
    "            f\"{self.base_url}/cse/\",\n",
    "            f\"{self.base_url}/ece/\",\n",
    "            f\"{self.base_url}/mech/\",\n",
    "            f\"{self.base_url}/civil/\",\n",
    "            f\"{self.base_url}/eee/\"\n",
    "        ]\n",
    "        \n",
    "        # Try to find department links from main page\n",
    "        main_soup = self.get_page_with_selenium(self.base_url)\n",
    "        if main_soup:\n",
    "            dept_links = main_soup.find_all('a', href=re.compile(r'/(cse|ece|mech|civil|eee|mba|csa)/'))\n",
    "            for link in dept_links:\n",
    "                dept_url = urljoin(self.base_url, link.get('href'))\n",
    "                if dept_url not in dept_urls:\n",
    "                    dept_urls.append(dept_url)\n",
    "        \n",
    "        # Scrape each department\n",
    "        for dept_url in dept_urls[:10]:  # Limit to avoid too many requests\n",
    "            try:\n",
    "                soup = self.get_page_with_requests(dept_url)\n",
    "                if soup:\n",
    "                    dept_name = self.extract_department_name(soup, dept_url)\n",
    "                    if dept_name:\n",
    "                        departments.append({\n",
    "                            'name': dept_name,\n",
    "                            'url': dept_url,\n",
    "                            'description': self.extract_department_description(soup)\n",
    "                        })\n",
    "                \n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error scraping department {dept_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Found {len(departments)} departments\")\n",
    "        return departments\n",
    "    \n",
    "    def scrape_courses_programs(self) -> List[Dict]:\n",
    "        \"\"\"Scrape courses and programs\"\"\"\n",
    "        logger.info(\"Scraping courses and programs...\")\n",
    "        \n",
    "        courses = []\n",
    "        \n",
    "        # Look for academic/courses pages\n",
    "        course_urls = [\n",
    "            f\"{self.base_url}/academics.aspx\",\n",
    "            f\"{self.base_url}/programmes.aspx\",\n",
    "            f\"{self.base_url}/courses.aspx\"\n",
    "        ]\n",
    "        \n",
    "        for course_url in course_urls:\n",
    "            try:\n",
    "                soup = self.get_page_with_selenium(course_url)\n",
    "                if soup:\n",
    "                    # Extract course information\n",
    "                    course_links = soup.find_all('a', href=re.compile(r'(course|program|degree)'))\n",
    "                    for link in course_links:\n",
    "                        course_name = link.get_text(strip=True)\n",
    "                        course_href = link.get('href')\n",
    "                        \n",
    "                        if course_name and len(course_name) > 5:\n",
    "                            courses.append({\n",
    "                                'name': course_name,\n",
    "                                'url': urljoin(self.base_url, course_href) if course_href else course_url,\n",
    "                                'level': self.extract_course_level(course_name),\n",
    "                                'department': self.extract_department_from_course(course_name)\n",
    "                            })\n",
    "                \n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error scraping courses from {course_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_courses = []\n",
    "        for course in courses:\n",
    "            if course['name'] not in seen:\n",
    "                seen.add(course['name'])\n",
    "                unique_courses.append(course)\n",
    "        \n",
    "        logger.info(f\"Found {len(unique_courses)} courses\")\n",
    "        return unique_courses[:50]  # Limit to 50 courses\n",
    "    \n",
    "    def scrape_faculty(self) -> List[Dict]:\n",
    "        \"\"\"Scrape faculty information\"\"\"\n",
    "        logger.info(\"Scraping faculty information...\")\n",
    "        \n",
    "        faculty = []\n",
    "        \n",
    "        # Look for faculty pages\n",
    "        faculty_urls = [\n",
    "            f\"{self.base_url}/faculty.aspx\",\n",
    "            f\"{self.base_url}/staff.aspx\"\n",
    "        ]\n",
    "        \n",
    "        for faculty_url in faculty_urls:\n",
    "            try:\n",
    "                soup = self.get_page_with_selenium(faculty_url)\n",
    "                if soup:\n",
    "                    # Extract faculty information\n",
    "                    faculty_elements = soup.find_all(['div', 'td', 'span'], text=re.compile(r'Dr\\.|Prof\\.|Mr\\.|Ms\\.'))\n",
    "                    \n",
    "                    for elem in faculty_elements:\n",
    "                        faculty_name = elem.get_text(strip=True)\n",
    "                        if len(faculty_name) > 3 and len(faculty_name) < 100:\n",
    "                            faculty.append({\n",
    "                                'name': faculty_name,\n",
    "                                'url': faculty_url,\n",
    "                                'department': self.extract_department_from_context(elem),\n",
    "                                'designation': self.extract_designation(faculty_name)\n",
    "                            })\n",
    "                \n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error scraping faculty from {faculty_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Found {len(faculty)} faculty members\")\n",
    "        return faculty[:100]  # Limit to 100 faculty\n",
    "    \n",
    "    def scrape_achievements(self) -> List[Dict]:\n",
    "        \"\"\"Scrape university achievements and awards\"\"\"\n",
    "        logger.info(\"Scraping achievements...\")\n",
    "        \n",
    "        achievements = []\n",
    "        \n",
    "        # From the main page content, extract achievements\n",
    "        soup = self.get_page_with_selenium(self.base_url)\n",
    "        if soup:\n",
    "            # Look for achievement-related text\n",
    "            achievement_keywords = ['award', 'ranking', 'recognition', 'prize', 'achievement']\n",
    "            \n",
    "            for keyword in achievement_keywords:\n",
    "                elements = soup.find_all(text=re.compile(keyword, re.I))\n",
    "                for elem in elements:\n",
    "                    if elem.parent:\n",
    "                        achievement_text = elem.parent.get_text(strip=True)\n",
    "                        if len(achievement_text) > 50:\n",
    "                            achievements.append({\n",
    "                                'title': achievement_text[:200],\n",
    "                                'type': keyword,\n",
    "                                'year': self.extract_year_from_text(achievement_text),\n",
    "                                'description': achievement_text[:500]\n",
    "                            })\n",
    "        \n",
    "        logger.info(f\"Found {len(achievements)} achievements\")\n",
    "        return achievements[:15]  # Limit to 15 achievements\n",
    "    \n",
    "    def extract_contact_info(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"Extract contact information\"\"\"\n",
    "        contact_info = {}\n",
    "        \n",
    "        # Extract phone numbers\n",
    "        phone_pattern = r'[\\+]?[0-9]{10,15}'\n",
    "        phones = re.findall(phone_pattern, str(soup))\n",
    "        if phones:\n",
    "            contact_info['phones'] = list(set(phones))\n",
    "        \n",
    "        # Extract email addresses\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, str(soup))\n",
    "        if emails:\n",
    "            contact_info['emails'] = list(set(emails))\n",
    "        \n",
    "        # Extract address\n",
    "        address_elem = soup.find(text=re.compile('address|location|campus', re.I))\n",
    "        if address_elem and address_elem.parent:\n",
    "            contact_info['address'] = address_elem.parent.get_text(strip=True)\n",
    "        \n",
    "        return contact_info\n",
    "    \n",
    "    # Helper methods\n",
    "    def extract_date_from_text(self, text: str) -> str:\n",
    "        \"\"\"Extract date from text\"\"\"\n",
    "        date_pattern = r'(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})|(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})'\n",
    "        match = re.search(date_pattern, text)\n",
    "        return match.group() if match else ''\n",
    "    \n",
    "    def extract_year_from_text(self, text: str) -> str:\n",
    "        \"\"\"Extract year from text\"\"\"\n",
    "        year_pattern = r'20\\d{2}'\n",
    "        match = re.search(year_pattern, text)\n",
    "        return match.group() if match else ''\n",
    "    \n",
    "    def extract_department_name(self, soup: BeautifulSoup, url: str) -> str:\n",
    "        \"\"\"Extract department name from page\"\"\"\n",
    "        # Try to get from title or h1\n",
    "        title_elem = soup.find('title')\n",
    "        if title_elem:\n",
    "            return title_elem.get_text(strip=True)\n",
    "        \n",
    "        h1_elem = soup.find('h1')\n",
    "        if h1_elem:\n",
    "            return h1_elem.get_text(strip=True)\n",
    "        \n",
    "        # Extract from URL\n",
    "        path = urlparse(url).path\n",
    "        return path.split('/')[-2] if len(path.split('/')) > 1 else 'Unknown'\n",
    "    \n",
    "    def extract_department_description(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract department description\"\"\"\n",
    "        # Look for description in common places\n",
    "        desc_selectors = ['.description', '#description', '.about', '#about']\n",
    "        for selector in desc_selectors:\n",
    "            elem = soup.select_one(selector)\n",
    "            if elem:\n",
    "                return elem.get_text(strip=True)[:500]\n",
    "        \n",
    "        # Get first paragraph\n",
    "        p_elem = soup.find('p')\n",
    "        if p_elem:\n",
    "            return p_elem.get_text(strip=True)[:500]\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def extract_course_level(self, course_name: str) -> str:\n",
    "        \"\"\"Extract course level (UG/PG/PhD)\"\"\"\n",
    "        course_name_lower = course_name.lower()\n",
    "        if any(term in course_name_lower for term in ['b.tech', 'b.e', 'bachelor', 'btech']):\n",
    "            return 'Undergraduate'\n",
    "        elif any(term in course_name_lower for term in ['m.tech', 'm.e', 'master', 'mtech', 'mba']):\n",
    "            return 'Postgraduate'\n",
    "        elif any(term in course_name_lower for term in ['ph.d', 'phd', 'doctorate']):\n",
    "            return 'Doctoral'\n",
    "        return 'Unknown'\n",
    "    \n",
    "    def extract_department_from_course(self, course_name: str) -> str:\n",
    "        \"\"\"Extract department from course name\"\"\"\n",
    "        course_name_lower = course_name.lower()\n",
    "        if 'computer' in course_name_lower or 'cse' in course_name_lower:\n",
    "            return 'Computer Science'\n",
    "        elif 'mechanical' in course_name_lower or 'mech' in course_name_lower:\n",
    "            return 'Mechanical'\n",
    "        elif 'electrical' in course_name_lower or 'eee' in course_name_lower:\n",
    "            return 'Electrical'\n",
    "        elif 'civil' in course_name_lower:\n",
    "            return 'Civil'\n",
    "        elif 'electronics' in course_name_lower or 'ece' in course_name_lower:\n",
    "            return 'Electronics'\n",
    "        return 'General'\n",
    "    \n",
    "    def extract_department_from_context(self, elem) -> str:\n",
    "        \"\"\"Extract department from element context\"\"\"\n",
    "        # Look for department indicators in parent elements\n",
    "        parent = elem.parent\n",
    "        if parent:\n",
    "            parent_text = parent.get_text().lower()\n",
    "            if 'cse' in parent_text or 'computer' in parent_text:\n",
    "                return 'Computer Science'\n",
    "            elif 'ece' in parent_text or 'electronics' in parent_text:\n",
    "                return 'Electronics'\n",
    "            elif 'mech' in parent_text or 'mechanical' in parent_text:\n",
    "                return 'Mechanical'\n",
    "        return 'Unknown'\n",
    "    \n",
    "    def extract_designation(self, name: str) -> str:\n",
    "        \"\"\"Extract designation from name\"\"\"\n",
    "        if name.startswith('Dr.'):\n",
    "            return 'Doctor'\n",
    "        elif name.startswith('Prof.'):\n",
    "            return 'Professor'\n",
    "        elif name.startswith('Mr.'):\n",
    "            return 'Mr.'\n",
    "        elif name.startswith('Ms.'):\n",
    "            return 'Ms.'\n",
    "        return 'Unknown'\n",
    "    \n",
    "    def scrape_all_data(self) -> KLUData:\n",
    "        \"\"\"Main scraping method\"\"\"\n",
    "        logger.info(\"Starting comprehensive KL University scraping...\")\n",
    "        \n",
    "        # Get main page for contact info\n",
    "        main_soup = self.get_page_with_selenium(self.base_url)\n",
    "        contact_info = self.extract_contact_info(main_soup) if main_soup else {}\n",
    "        \n",
    "        # Scrape all sections\n",
    "        notifications = self.scrape_main_page_notifications()\n",
    "        departments = self.scrape_departments()\n",
    "        courses = self.scrape_courses_programs()\n",
    "        faculty = self.scrape_faculty()\n",
    "        achievements = self.scrape_achievements()\n",
    "        \n",
    "        # Create data object\n",
    "        klu_data = KLUData(\n",
    "            notifications=notifications,\n",
    "            departments=departments,\n",
    "            courses=courses,\n",
    "            faculty=faculty,\n",
    "            news=notifications,  # Notifications can serve as news\n",
    "            achievements=achievements,\n",
    "            contact_info=contact_info,\n",
    "            scraped_at=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Scraping completed successfully!\")\n",
    "        return klu_data\n",
    "    \n",
    "    def save_data(self, data: KLUData, output_dir: str = \"klu_scraped_data\"):\n",
    "        \"\"\"Save scraped data\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_path = os.path.join(output_dir, \"klu_data.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data.__dict__, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save individual components as CSV\n",
    "        if data.notifications:\n",
    "            pd.DataFrame(data.notifications).to_csv(\n",
    "                os.path.join(output_dir, \"notifications.csv\"), index=False\n",
    "            )\n",
    "        \n",
    "        if data.departments:\n",
    "            pd.DataFrame(data.departments).to_csv(\n",
    "                os.path.join(output_dir, \"departments.csv\"), index=False\n",
    "            )\n",
    "        \n",
    "        if data.courses:\n",
    "            pd.DataFrame(data.courses).to_csv(\n",
    "                os.path.join(output_dir, \"courses.csv\"), index=False\n",
    "            )\n",
    "        \n",
    "        if data.faculty:\n",
    "            pd.DataFrame(data.faculty).to_csv(\n",
    "                os.path.join(output_dir, \"faculty.csv\"), index=False\n",
    "            )\n",
    "        \n",
    "        if data.achievements:\n",
    "            pd.DataFrame(data.achievements).to_csv(\n",
    "                os.path.join(output_dir, \"achievements.csv\"), index=False\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Data saved to {output_dir}\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    try:\n",
    "        # Initialize scraper\n",
    "        scraper = KLUniversityScraper(delay=2.0)\n",
    "        \n",
    "        # Scrape all data\n",
    "        klu_data = scraper.scrape_all_data()\n",
    "        \n",
    "        # Save data\n",
    "        scraper.save_data(klu_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüè´ KL University Scraping Summary:\")\n",
    "        print(f\"üì¢ Notifications: {len(klu_data.notifications)}\")\n",
    "        print(f\"üè¢ Departments: {len(klu_data.departments)}\")\n",
    "        print(f\"üìö Courses: {len(klu_data.courses)}\")\n",
    "        print(f\"üë®‚Äçüè´ Faculty: {len(klu_data.faculty)}\")\n",
    "        print(f\"üèÜ Achievements: {len(klu_data.achievements)}\")\n",
    "        print(f\"üìû Contact Info: {klu_data.contact_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28b4beb-a056-460f-b7ec-d418adc9374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = requests.get(\"https://www.kluniversity.in/\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# e.g. list all notification titles\n",
    "for li in soup.select(\"ul.notifications li a\"):\n",
    "    print(li.get_text(strip=True), li[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35867f0f-7302-4d72-832b-3a0353b19ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
