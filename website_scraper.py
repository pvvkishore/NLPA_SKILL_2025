import requests
from bs4 import BeautifulSoup
import json
import csv
import pandas as pd
from urllib.parse import urljoin, urlparse
import time
import re
from typing import Dict, List, Optional, Set
import os
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
import hashlib

@dataclass
class ScrapedContent:
    """Data class to store scraped content"""
    url: str
    title: str
    content: str
    content_type: List[str]
    word_count: int
    scraped_at: str
    page_hash: str
    metadata: Dict

class WebsiteScraper:
    def __init__(self, config_file: str = 'scraper_config.json'):
        """Initialize the scraper with configuration from analyzer"""
        self.config = self.load_config(config_file)
        self.scraped_data = []
        self.failed_urls = []
        self.session = requests.Session()
        self.lock = threading.Lock()
        
        # Setup headers to mimic real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
        
        # Setup logging
        self.setup_logging()
        
        # Extract settings from config
        settings = self.config.get('scraping_settings', {})
        self.delay = settings.get('delay_between_requests', 1)
        self.max_workers = settings.get('max_concurrent_requests', 3)
        self.timeout = settings.get('timeout', 30)
        self.retry_attempts = settings.get('retry_attempts', 3)
        
        # Content extraction rules
        self.content_rules = self.config.get('content_extraction_rules', {})
        self.primary_selectors = self.content_rules.get('primary_content_selectors', ['main', 'article'])
        self.text_selectors = self.content_rules.get('text_selectors', ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        self.exclude_selectors = self.content_rules.get('exclude_selectors', ['nav', 'footer'])
        
        print(f"Initialized scraper for {self.config['base_url']}")
        print(f"Found {len(self.config['scraping_targets']['all_discovered_urls'])} URLs to scrape")
    
    def load_config(self, config_file: str) -> Dict:
        """Load configuration generated by website analyzer"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"Configuration file {config_file} not found. Please run the website analyzer first.")
        except json.JSONDecodeError:
            raise ValueError(f"Invalid JSON in configuration file {config_file}")
    
    def setup_logging(self):
        """Setup logging for the scraper"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize extracted text"""
        if not text:
            return ""
        
        # Remove extra whitespace and normalize
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Remove common unwanted patterns
        text = re.sub(r'(Share|Tweet|Like|Follow|Subscribe|Click here|Read more)(\s|$)', '', text, flags=re.IGNORECASE)
        
        # Remove email addresses and phone numbers for privacy
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
        text = re.sub(r'(\+\d{1,3}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}', '[PHONE]', text)
        
        # Remove excessive punctuation
        text = re.sub(r'[.]{3,}', '...', text)
        text = re.sub(r'[-]{3,}', '---', text)
        
        return text.strip()
    
    def extract_content_from_page(self, soup: BeautifulSoup, url: str) -> Dict:
        """Extract main content from a webpage"""
        content_parts = []
        title = ""
        metadata = {}
        
        # Extract title
        title_tag = soup.find('title')
        if title_tag:
            title = self.clean_text(title_tag.get_text())
        
        # Extract meta description
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc:
            metadata['description'] = meta_desc.get('content', '')
        
        # Extract meta keywords
        meta_keywords = soup.find('meta', {'name': 'keywords'})
        if meta_keywords:
            metadata['keywords'] = meta_keywords.get('content', '')
        
        # Remove unwanted elements first
        for selector in self.exclude_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        # Try to find main content using primary selectors
        main_content_found = False
        for selector in self.primary_selectors:
            content_elements = soup.select(selector)
            if content_elements:
                for element in content_elements:
                    text = self.clean_text(element.get_text())
                    if len(text) > 100:  # Only substantial content
                        content_parts.append(text)
                        main_content_found = True
        
        # If no main content found, extract from text selectors
        if not main_content_found:
            for selector in self.text_selectors:
                elements = soup.find_all(selector)
                for element in elements:
                    text = self.clean_text(element.get_text())
                    if len(text) > 20:  # Minimum text length
                        content_parts.append(text)
        
        # Combine all content
        full_content = '\n\n'.join(content_parts)
        
        return {
            'title': title,
            'content': full_content,
            'word_count': len(full_content.split()),
            'metadata': metadata
        }
    
    def scrape_single_page(self, url: str) -> Optional[ScrapedContent]:
        """Scrape content from a single page"""
        for attempt in range(self.retry_attempts):
            try:
                self.logger.info(f"Scraping: {url} (attempt {attempt + 1})")
                
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                # Check if it's HTML content
                if 'text/html' not in response.headers.get('content-type', ''):
                    self.logger.warning(f"Non-HTML content at {url}")
                    return None
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract content
                extracted_data = self.extract_content_from_page(soup, url)
                
                if not extracted_data['content'] or len(extracted_data['content']) < 100:
                    self.logger.warning(f"Insufficient content found at {url}")
                    return None
                
                # Find content types for this page
                content_types = []
                page_targets = self.config['scraping_targets']['high_priority_pages']
                for page_target in page_targets:
                    if page_target['url'] == url:
                        content_types = page_target.get('content_types', [])
                        break
                
                # Create content hash for deduplication
                content_hash = hashlib.md5(extracted_data['content'].encode()).hexdigest()
                
                scraped_content = ScrapedContent(
                    url=url,
                    title=extracted_data['title'],
                    content=extracted_data['content'],
                    content_type=content_types,
                    word_count=extracted_data['word_count'],
                    scraped_at=datetime.now().isoformat(),
                    page_hash=content_hash,
                    metadata=extracted_data['metadata']
                )
                
                time.sleep(self.delay)  # Respect delay
                return scraped_content
                
            except requests.exceptions.RequestException as e:
                self.logger.error(f"Request failed for {url}: {e}")
                if attempt == self.retry_attempts - 1:
                    self.failed_urls.append({'url': url, 'error': str(e)})
                time.sleep(2 ** attempt)  # Exponential backoff
                
            except Exception as e:
                self.logger.error(f"Unexpected error scraping {url}: {e}")
                if attempt == self.retry_attempts - 1:
                    self.failed_urls.append({'url': url, 'error': str(e)})
                break
        
        return None
    
    def scrape_all_pages(self, max_pages: Optional[int] = None) -> List[ScrapedContent]:
        """Scrape all discovered pages"""
        # Get list of URLs to scrape
        all_urls = self.config['scraping_targets']['all_discovered_urls']
        
        # Prioritize high-value pages
        high_priority_urls = [page['url'] for page in self.config['scraping_targets']['high_priority_pages']]
        
        # Combine and deduplicate URLs, prioritizing high-value pages
        urls_to_scrape = list(dict.fromkeys(high_priority_urls + all_urls))
        
        if max_pages:
            urls_to_scrape = urls_to_scrape[:max_pages]
        
        self.logger.info(f"Starting to scrape {len(urls_to_scrape)} pages")
        
        # Use ThreadPoolExecutor for concurrent scraping
        scraped_contents = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all scraping tasks
            future_to_url = {
                executor.submit(self.scrape_single_page, url): url 
                for url in urls_to_scrape
            }
            
            # Collect results
            for i, future in enumerate(as_completed(future_to_url)):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            scraped_contents.append(result)
                        self.logger.info(f"Successfully scraped {url} ({i+1}/{len(urls_to_scrape)})")
                    else:
                        self.logger.warning(f"No content extracted from {url}")
                        
                except Exception as e:
                    self.logger.error(f"Error processing {url}: {e}")
                    with self.lock:
                        self.failed_urls.append({'url': url, 'error': str(e)})
        
        self.scraped_data = scraped_contents
        self.logger.info(f"Scraping completed. Successfully scraped {len(scraped_contents)} pages")
        
        return scraped_contents
    
    def deduplicate_content(self) -> List[ScrapedContent]:
        """Remove duplicate content based on content hash"""
        seen_hashes = set()
        unique_content = []
        
        for content in self.scraped_data:
            if content.page_hash not in seen_hashes:
                seen_hashes.add(content.page_hash)
                unique_content.append(content)
            else:
                self.logger.info(f"Removed duplicate content: {content.url}")
        
        self.logger.info(f"Removed {len(self.scraped_data) - len(unique_content)} duplicate pages")
        self.scraped_data = unique_content
        return unique_content
    
    def save_to_json(self, filename: str = None) -> str:
        """Save scraped data to JSON file"""
        if not filename:
            domain = urlparse(self.config['base_url']).netloc.replace('.', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{domain}_scraped_data_{timestamp}.json"
        
        # Prepare data for JSON serialization
        json_data = {
            'scraping_metadata': {
                'base_url': self.config['base_url'],
                'scraped_at': datetime.now().isoformat(),
                'total_pages_scraped': len(self.scraped_data),
                'failed_pages': len(self.failed_urls),
                'scraper_config': self.config.get('site_analysis', {})
            },
            'scraped_pages': [
                {
                    'url': content.url,
                    'title': content.title,
                    'content': content.content,
                    'content_type': content.content_type,
                    'word_count': content.word_count,
                    'scraped_at': content.scraped_at,
                    'page_hash': content.page_hash,
                    'metadata': content.metadata
                }
                for content in self.scraped_data
            ],
            'failed_urls': self.failed_urls
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Data saved to {filename}")
        return filename
    
    def save_to_csv(self, filename: str = None) -> str:
        """Save scraped data to CSV file"""
        if not filename:
            domain = urlparse(self.config['base_url']).netloc.replace('.', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{domain}_scraped_data_{timestamp}.csv"
        
        # Prepare data for CSV
        csv_data = []
        for content in self.scraped_data:
            csv_data.append({
                'url': content.url,
                'title': content.title,
                'content': content.content,
                'content_type': '; '.join(content.content_type),
                'word_count': content.word_count,
                'scraped_at': content.scraped_at,
                'page_hash': content.page_hash,
                'meta_description': content.metadata.get('description', ''),
                'meta_keywords': content.metadata.get('keywords', '')
            })
        
        # Save using pandas for better handling of large text
        df = pd.DataFrame(csv_data)
        df.to_csv(filename, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)
        
        self.logger.info(f"Data saved to {filename}")
        return filename
    
    def generate_statistics(self) -> Dict:
        """Generate statistics about the scraped data"""
        if not self.scraped_data:
            return {}
        
        # Basic statistics
        total_pages = len(self.scraped_data)
        total_words = sum(content.word_count for content in self.scraped_data)
        avg_words_per_page = total_words / total_pages if total_pages > 0 else 0
        
        # Content type distribution
        content_type_counts = {}
        for content in self.scraped_data:
            for content_type in content.content_type:
                content_type_counts[content_type] = content_type_counts.get(content_type, 0) + 1
        
        # Word count distribution
        word_counts = [content.word_count for content in self.scraped_data]
        word_counts.sort()
        
        stats = {
            'total_pages_scraped': total_pages,
            'total_words': total_words,
            'average_words_per_page': round(avg_words_per_page, 2),
            'content_type_distribution': content_type_counts,
            'word_count_stats': {
                'min': min(word_counts) if word_counts else 0,
                'max': max(word_counts) if word_counts else 0,
                'median': word_counts[len(word_counts)//2] if word_counts else 0
            },
            'failed_pages': len(self.failed_urls),
            'success_rate': f"{(total_pages / (total_pages + len(self.failed_urls)) * 100):.1f}%" if total_pages + len(self.failed_urls) > 0 else "0%"
        }
        
        return stats
    
    def create_text_dataset(self, output_dir: str = "scraped_dataset") -> Dict[str, str]:
        """Create a complete text dataset with multiple output formats"""
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Remove duplicates
        self.deduplicate_content()
        
        # Save in different formats
        json_file = os.path.join(output_dir, self.save_to_json().split('/')[-1])
        csv_file = os.path.join(output_dir, self.save_to_csv().split('/')[-1])
        
        # Move files to output directory
        if not json_file.startswith(output_dir):
            os.rename(self.save_to_json(), json_file)
        if not csv_file.startswith(output_dir):
            os.rename(self.save_to_csv(), csv_file)
        
        # Generate and save statistics
        stats = self.generate_statistics()
        stats_file = os.path.join(output_dir, 'scraping_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # Create a simple text file with all content
        txt_file = os.path.join(output_dir, 'all_content.txt')
        with open(txt_file, 'w', encoding='utf-8') as f:
            for i, content in enumerate(self.scraped_data, 1):
                f.write(f"=== Page {i}: {content.title} ===\n")
                f.write(f"URL: {content.url}\n")
                f.write(f"Word Count: {content.word_count}\n")
                f.write(f"Content Types: {', '.join(content.content_type)}\n")
                f.write(f"Scraped At: {content.scraped_at}\n")
                f.write("-" * 80 + "\n")
                f.write(content.content)
                f.write("\n" + "=" * 80 + "\n\n")
        
        # Print summary
        print(f"\n=== Text Dataset Creation Complete ===")
        print(f"Output directory: {output_dir}")
        print(f"Files created:")
        print(f"  - {json_file} (JSON format)")
        print(f"  - {csv_file} (CSV format)")
        print(f"  - {txt_file} (Plain text)")
        print(f"  - {stats_file} (Statistics)")
        print(f"\nStatistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        return {
            'json_file': json_file,
            'csv_file': csv_file,
            'txt_file': txt_file,
            'stats_file': stats_file,
            'output_directory': output_dir
        }

def scrape_website_from_config(config_file: str = 'scraper_config.json', 
                              output_dir: str = "scraped_dataset",
                              max_pages: Optional[int] = None) -> Dict[str, str]:
    """
    Main function to scrape website using analyzer configuration
    
    Args:
        config_file: Path to the configuration file generated by website analyzer
        output_dir: Directory to save the scraped dataset
        max_pages: Maximum number of pages to scrape (None for all)
    
    Returns:
        Dictionary with paths to generated files
    """
    
    # Initialize scraper
    scraper = WebsiteScraper(config_file)
    
    # Scrape all pages
    scraped_data = scraper.scrape_all_pages(max_pages)
    
    if not scraped_data:
        print("No content was successfully scraped!")
        return {}
    
    # Create dataset files
    return scraper.create_text_dataset(output_dir)

# Example usage
if __name__ == "__main__":
    # Make sure you have run the website analyzer first to generate scraper_config.json
    try:
        # Scrape the website using the configuration from analyzer
        results = scrape_website_from_config(
            config_file='scraper_config.json',  # Output from your analyzer
            output_dir='scraped_dataset',       # Where to save results
            max_pages=None                      # Scrape all pages (set to number to limit)
        )
        
        if results:
            print(f"\nScraping completed successfully!")
            print(f"Check the '{results['output_directory']}' folder for all generated files.")
        else:
            print("Scraping failed or no content was extracted.")
            
    except FileNotFoundError:
        print("Error: scraper_config.json not found.")
        print("Please run the website analyzer first to generate the configuration file.")
    except Exception as e:
        print(f"Error during scraping: {e}")
