{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oI025Jq8GjMU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import mimetypes\n",
    "\n",
    "class WebsiteContentAnalyzer:\n",
    "    def __init__(self, base_url: str, max_pages: int = 100):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        self.max_pages = max_pages\n",
    "        self.visited_urls = set()\n",
    "        self.content_locations = defaultdict(list)\n",
    "        self.document_links = []\n",
    "        self.content_patterns = {}\n",
    "        self.site_structure = {}\n",
    "\n",
    "        # Headers to mimic a real browser\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    def discover_content_locations(self) -> Dict:\n",
    "        \"\"\"Main method to analyze website and discover all content locations\"\"\"\n",
    "        print(f\"Starting analysis of {self.base_url}\")\n",
    "\n",
    "        # Step 1: Discover all pages and structure\n",
    "        self._crawl_sitemap()\n",
    "        self._discover_pages()\n",
    "\n",
    "        # Step 2: Analyze content patterns\n",
    "        self._analyze_content_patterns()\n",
    "\n",
    "        # Step 3: Find document links\n",
    "        self._find_document_links()\n",
    "\n",
    "        # Step 4: Analyze site structure\n",
    "        self._analyze_site_structure()\n",
    "\n",
    "        # Step 5: Generate scraper configuration\n",
    "        return self._generate_scraper_config()\n",
    "\n",
    "    def _crawl_sitemap(self):\n",
    "        \"\"\"Try to find and parse sitemap.xml\"\"\"\n",
    "        sitemap_urls = [\n",
    "            f\"{self.base_url}/sitemap.xml\",\n",
    "            f\"{self.base_url}/sitemap_index.xml\",\n",
    "            f\"{self.base_url}/robots.txt\"\n",
    "        ]\n",
    "\n",
    "        for sitemap_url in sitemap_urls:\n",
    "            try:\n",
    "                response = requests.get(sitemap_url, headers=self.headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    if 'sitemap.xml' in sitemap_url:\n",
    "                        self._parse_sitemap(response.text)\n",
    "                    elif 'robots.txt' in sitemap_url:\n",
    "                        self._parse_robots_txt(response.text)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not access {sitemap_url}: {e}\")\n",
    "\n",
    "    def _parse_sitemap(self, sitemap_content: str):\n",
    "        \"\"\"Parse sitemap XML to find all URLs\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(sitemap_content, 'xml')\n",
    "            urls = soup.find_all('loc')\n",
    "            for url in urls:\n",
    "                if url.text and self.domain in url.text:\n",
    "                    self.content_locations['sitemap_urls'].append(url.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sitemap: {e}\")\n",
    "\n",
    "    def _parse_robots_txt(self, robots_content: str):\n",
    "        \"\"\"Parse robots.txt to find sitemap references and allowed paths\"\"\"\n",
    "        lines = robots_content.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Sitemap:'):\n",
    "                sitemap_url = line.split(':', 1)[1].strip()\n",
    "                try:\n",
    "                    response = requests.get(sitemap_url, headers=self.headers, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        self._parse_sitemap(response.text)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    def _discover_pages(self):\n",
    "        \"\"\"Crawl the website to discover all accessible pages\"\"\"\n",
    "        to_visit = [self.base_url]\n",
    "        visited_count = 0\n",
    "\n",
    "        while to_visit and visited_count < self.max_pages:\n",
    "            current_url = to_visit.pop(0)\n",
    "\n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(current_url, headers=self.headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    self.visited_urls.add(current_url)\n",
    "                    visited_count += 1\n",
    "\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Find all internal links\n",
    "                    links = soup.find_all('a', href=True)\n",
    "                    for link in links:\n",
    "                        href = link['href']\n",
    "                        full_url = urljoin(current_url, href)\n",
    "\n",
    "                        # Only follow internal links\n",
    "                        if self.domain in full_url and full_url not in self.visited_urls:\n",
    "                            to_visit.append(full_url)\n",
    "\n",
    "                    # Analyze this page's content\n",
    "                    self._analyze_page_content(current_url, soup)\n",
    "\n",
    "                    time.sleep(0.5)  # Be respectful\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing {current_url}: {e}\")\n",
    "\n",
    "    def _analyze_page_content(self, url: str, soup: BeautifulSoup):\n",
    "        \"\"\"Analyze individual page content structure\"\"\"\n",
    "        page_info = {\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'content_areas': [],\n",
    "            'document_links': [],\n",
    "            'content_types': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            page_info['title'] = title_tag.get_text().strip()\n",
    "\n",
    "        # Identify main content areas\n",
    "        content_selectors = [\n",
    "            ('main_content', ['main', 'article', '.content', '.post', '.entry']),\n",
    "            ('navigation', ['nav', '.navigation', '.menu', '.nav']),\n",
    "            ('sidebar', ['.sidebar', '.widget', 'aside']),\n",
    "            ('header', ['header', '.header']),\n",
    "            ('footer', ['footer', '.footer'])\n",
    "        ]\n",
    "\n",
    "        for area_type, selectors in content_selectors:\n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text_content = element.get_text().strip()\n",
    "                    if len(text_content) > 50:  # Only substantial content\n",
    "                        page_info['content_areas'].append({\n",
    "                            'type': area_type,\n",
    "                            'selector': selector,\n",
    "                            'text_length': len(text_content),\n",
    "                            'html_structure': str(element)[:200] + '...'\n",
    "                        })\n",
    "\n",
    "        # Find content patterns\n",
    "        self._identify_content_patterns(soup, page_info)\n",
    "\n",
    "        # Find document links on this page\n",
    "        self._find_page_documents(soup, url, page_info)\n",
    "\n",
    "        # Store page analysis\n",
    "        self.content_locations['pages'].append(page_info)\n",
    "\n",
    "    def _identify_content_patterns(self, soup: BeautifulSoup, page_info: Dict):\n",
    "        \"\"\"Identify common content patterns and structures\"\"\"\n",
    "        patterns = {\n",
    "            'headings': len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),\n",
    "            'paragraphs': len(soup.find_all('p')),\n",
    "            'lists': len(soup.find_all(['ul', 'ol'])),\n",
    "            'tables': len(soup.find_all('table')),\n",
    "            'forms': len(soup.find_all('form')),\n",
    "            'images': len(soup.find_all('img')),\n",
    "            'videos': len(soup.find_all(['video', 'iframe']))\n",
    "        }\n",
    "\n",
    "        # Check for specific content types\n",
    "        content_indicators = {\n",
    "            'blog_post': bool(soup.find_all(class_=re.compile(r'post|article|blog', re.I))),\n",
    "            'product_page': bool(soup.find_all(class_=re.compile(r'product|item|catalog', re.I))),\n",
    "            'documentation': bool(soup.find_all(class_=re.compile(r'doc|guide|manual', re.I))),\n",
    "            'faq': bool(soup.find_all(class_=re.compile(r'faq|question|answer', re.I))),\n",
    "            'forum': bool(soup.find_all(class_=re.compile(r'forum|discussion|thread', re.I)))\n",
    "        }\n",
    "\n",
    "        page_info['content_patterns'] = patterns\n",
    "        page_info['content_types'] = [k for k, v in content_indicators.items() if v]\n",
    "\n",
    "    def _find_page_documents(self, soup: BeautifulSoup, base_url: str, page_info: Dict):\n",
    "        \"\"\"Find downloadable documents on this page\"\"\"\n",
    "        document_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.csv']\n",
    "\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "\n",
    "            # Check if it's a document\n",
    "            for ext in document_extensions:\n",
    "                if ext in href.lower():\n",
    "                    doc_info = {\n",
    "                        'url': full_url,\n",
    "                        'type': ext,\n",
    "                        'title': link.get_text().strip(),\n",
    "                        'context': str(link.parent)[:100] + '...' if link.parent else ''\n",
    "                    }\n",
    "                    page_info['document_links'].append(doc_info)\n",
    "                    self.document_links.append(doc_info)\n",
    "\n",
    "    def _find_document_links(self):\n",
    "        \"\"\"Comprehensive document discovery across the site\"\"\"\n",
    "        common_doc_paths = [\n",
    "            '/downloads/', '/docs/', '/documentation/', '/resources/',\n",
    "            '/files/', '/assets/', '/media/', '/papers/', '/reports/'\n",
    "        ]\n",
    "\n",
    "        for path in common_doc_paths:\n",
    "            try:\n",
    "                test_url = f\"{self.base_url}{path}\"\n",
    "                response = requests.get(test_url, headers=self.headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    self._find_page_documents(soup, test_url, {'document_links': []})\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def _analyze_content_patterns(self):\n",
    "        \"\"\"Analyze overall content patterns across the site\"\"\"\n",
    "        all_patterns = defaultdict(list)\n",
    "\n",
    "        for page in self.content_locations['pages']:\n",
    "            for pattern_type, count in page.get('content_patterns', {}).items():\n",
    "                all_patterns[pattern_type].append(count)\n",
    "\n",
    "        # Calculate averages and identify high-content pages\n",
    "        pattern_summary = {}\n",
    "        for pattern_type, counts in all_patterns.items():\n",
    "            if counts:\n",
    "                pattern_summary[pattern_type] = {\n",
    "                    'average': sum(counts) / len(counts),\n",
    "                    'max': max(counts),\n",
    "                    'total': sum(counts)\n",
    "                }\n",
    "\n",
    "        self.content_patterns = pattern_summary\n",
    "\n",
    "    def _analyze_site_structure(self):\n",
    "        \"\"\"Analyze overall site structure and organization\"\"\"\n",
    "        url_patterns = defaultdict(list)\n",
    "        content_types = defaultdict(int)\n",
    "\n",
    "        for page in self.content_locations['pages']:\n",
    "            # Analyze URL patterns\n",
    "            parsed_url = urlparse(page['url'])\n",
    "            path_parts = [part for part in parsed_url.path.split('/') if part]\n",
    "\n",
    "            if path_parts:\n",
    "                url_patterns[path_parts[0]].append(page['url'])\n",
    "\n",
    "            # Count content types\n",
    "            for content_type in page.get('content_types', []):\n",
    "                content_types[content_type] += 1\n",
    "\n",
    "        self.site_structure = {\n",
    "            'url_patterns': dict(url_patterns),\n",
    "            'content_type_distribution': dict(content_types),\n",
    "            'total_pages': len(self.content_locations['pages']),\n",
    "            'total_documents': len(self.document_links)\n",
    "        }\n",
    "\n",
    "    def _generate_scraper_config(self) -> Dict:\n",
    "        \"\"\"Generate comprehensive configuration for the scraper\"\"\"\n",
    "\n",
    "        # Prioritize pages by content richness\n",
    "        high_value_pages = []\n",
    "        for page in self.content_locations['pages']:\n",
    "            content_score = 0\n",
    "            patterns = page.get('content_patterns', {})\n",
    "\n",
    "            # Score based on content richness\n",
    "            content_score += patterns.get('paragraphs', 0) * 2\n",
    "            content_score += patterns.get('headings', 0) * 3\n",
    "            content_score += patterns.get('lists', 0) * 1\n",
    "            content_score += len(page.get('content_types', [])) * 5\n",
    "\n",
    "            if content_score > 10:  # Threshold for valuable content\n",
    "                high_value_pages.append({\n",
    "                    'url': page['url'],\n",
    "                    'title': page['title'],\n",
    "                    'score': content_score,\n",
    "                    'content_types': page.get('content_types', []),\n",
    "                    'selectors': self._generate_content_selectors(page)\n",
    "                })\n",
    "\n",
    "        # Sort by content score\n",
    "        high_value_pages.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        # Generate scraper configuration\n",
    "        scraper_config = {\n",
    "            'base_url': self.base_url,\n",
    "            'domain': self.domain,\n",
    "            'site_analysis': {\n",
    "                'total_pages_found': len(self.content_locations['pages']),\n",
    "                'high_value_pages': len(high_value_pages),\n",
    "                'document_count': len(self.document_links),\n",
    "                'content_patterns': self.content_patterns,\n",
    "                'site_structure': self.site_structure\n",
    "            },\n",
    "            'scraping_targets': {\n",
    "                'high_priority_pages': high_value_pages[:50],  # Top 50 pages\n",
    "                'document_links': self.document_links,\n",
    "                'all_discovered_urls': [page['url'] for page in self.content_locations['pages']]\n",
    "            },\n",
    "            'content_extraction_rules': {\n",
    "                'primary_content_selectors': [\n",
    "                    'main', 'article', '.content', '.post', '.entry-content',\n",
    "                    '.article-body', '.post-content', '.page-content'\n",
    "                ],\n",
    "                'text_selectors': ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'],\n",
    "                'exclude_selectors': [\n",
    "                    'nav', 'footer', '.sidebar', '.advertisement', '.ads',\n",
    "                    '.menu', '.navigation', 'header', '.header'\n",
    "                ]\n",
    "            },\n",
    "            'document_extraction': {\n",
    "                'pdf_links': [doc for doc in self.document_links if '.pdf' in doc['type']],\n",
    "                'office_docs': [doc for doc in self.document_links if doc['type'] in ['.doc', '.docx', '.ppt', '.pptx']],\n",
    "                'data_files': [doc for doc in self.document_links if doc['type'] in ['.csv', '.txt', '.json']]\n",
    "            },\n",
    "            'scraping_settings': {\n",
    "                'delay_between_requests': 1,\n",
    "                'max_concurrent_requests': 3,\n",
    "                'timeout': 30,\n",
    "                'retry_attempts': 3,\n",
    "                'respect_robots_txt': True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return scraper_config\n",
    "\n",
    "    def _generate_content_selectors(self, page_info: Dict) -> List[str]:\n",
    "        \"\"\"Generate specific content selectors for this page\"\"\"\n",
    "        selectors = []\n",
    "\n",
    "        for content_area in page_info.get('content_areas', []):\n",
    "            if content_area['type'] == 'main_content' and content_area['text_length'] > 200:\n",
    "                selectors.append(content_area['selector'])\n",
    "\n",
    "        return selectors if selectors else ['main', 'article', '.content']\n",
    "\n",
    "    def save_config(self, filename: str = 'scraper_config.json'):\n",
    "        \"\"\"Save the scraper configuration to a JSON file\"\"\"\n",
    "        config = self.discover_content_locations()\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Scraper configuration saved to {filename}\")\n",
    "        return config\n",
    "\n",
    "# Usage example\n",
    "def analyze_website(url: str, max_pages: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Main function to analyze a website and generate scraper configuration\n",
    "\n",
    "    Args:\n",
    "        url: The base URL of the website to analyze\n",
    "        max_pages: Maximum number of pages to crawl for analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing complete scraper configuration\n",
    "    \"\"\"\n",
    "    analyzer = WebsiteContentAnalyzer(url, max_pages)\n",
    "    config = analyzer.save_config()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== Website Analysis Complete ===\")\n",
    "    print(f\"Website: {url}\")\n",
    "    print(f\"Pages analyzed: {config['site_analysis']['total_pages_found']}\")\n",
    "    print(f\"High-value pages: {config['site_analysis']['high_value_pages']}\")\n",
    "    print(f\"Documents found: {config['site_analysis']['document_count']}\")\n",
    "    print(f\"Content types discovered: {list(config['site_analysis']['site_structure']['content_type_distribution'].keys())}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Analyze a website\n",
    "    website_url = \"https://kluniversity.in\"  # Replace with target website\n",
    "    config = analyze_website(website_url, max_pages=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNL0YC0GO3pUTGi57JQDE+g",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
