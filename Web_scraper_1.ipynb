{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a571a-63f7-418e-aff1-c616bc1bdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 selenium pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ac468f-d49d-4b76-a20a-fade61e83e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:17:00,717 - INFO - Starting to scrape https://www.kluniversity.in\n",
      "C:\\Users\\Dr.PVVK\\AppData\\Local\\Temp\\ipykernel_33868\\1233661843.py:142: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  addr_elem = soup.find(text=re.compile(keyword, re.I))\n",
      "2025-07-18 16:17:23,396 - INFO - Scraping completed. Found 1 departments, 2 courses, 2 faculty, 8 news items\n",
      "2025-07-18 16:17:23,402 - INFO - Data saved to scraped_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ« University: About Us\n",
      "ðŸ“ Description: ...\n",
      "ðŸ¢ Departments: 1\n",
      "ðŸ“š Courses: 2\n",
      "ðŸ‘¨â€ðŸ« Faculty: 2\n",
      "ðŸ“° News: 8\n",
      "ðŸ“ž Contact: {'phones': ['91781592683', '+91799799838', '9849519527', '18462450823', '+91799799572', '15461422121', '7815901716'], 'address': '.trigger_popup {\\r\\n                    transform: rotate(90deg) !important;\\r\\n                    position: fixed; \\r\\n                    top: 39%; \\r\\n                   right:-46px;\\r\\n                    z-index: 999;\\r\\n                    cursor: pointer;\\r\\n                    background-color: #b8292f;\\r\\n                    border-color: #b8292f;\\r\\n                    border-radius: 5px;\\r\\n                    border-bottom-right-radius: 0;\\r\\n                    border-bottom-left-radius: 0; \\r\\n                    padding: 10px 12px;\\r\\n                    font-size: 18px;\\r\\n                    color: #fff;\\r\\n                    line-height: 1.33;         \\r\\n                    /* visibility: hidden;          */\\r\\n                }\\r\\n.trigger_popup:hover {\\r\\n                    background-color: #d63232;\\r\\n                    border-color: #d63232;\\r\\n                }\\r\\n\\r\\n/* The Modal (background) */\\r\\n.modal {\\r\\n  display: none; /* Hidden by default */\\r\\n  position: fixed; /* Stay in place */\\r\\n  z-index: 9999; /* Sit on top */\\r\\n  padding-top: 35px; /* Location of the box */\\r\\n  left: 0;\\r\\n  top: 0;\\r\\n  width: 100%; /* Full width */\\r\\n  height: 100%; /* Full height */\\r\\n  overflow: auto; /* Enable scroll if needed */\\r\\n  background-color: rgb(0,0,0); /* Fallback color */\\r\\n  background-color: rgba(0,0,0,0.4); /* Black w/ opacity */\\r\\n}\\r\\n\\r\\n/* Modal Content */\\r\\n.modal-content {\\r\\n  background-color: transparent;\\r\\n  margin: auto;\\r\\n  padding: 0;\\r\\n  border: 0px solid #888;\\r\\n  max-width: 390px;\\r\\n  position: relative;\\r\\n}\\r\\n\\r\\n/* The Close Button */\\r\\n.close {\\r\\n    color: #c1c1c1;\\r\\n    float: right;\\r\\n    font-size: 30px;\\r\\n    font-weight: bold;\\r\\n    position: absolute;\\r\\n    right: 15px;\\r\\n    z-index: 9999;\\r\\n    top: 2px;\\r\\n}\\r\\n\\r\\n.close:hover,\\r\\n.close:focus {\\r\\n  color: #797878;\\r\\n  text-decoration: none;\\r\\n  cursor: pointer;\\r\\n}\\r\\n.head_text {\\r\\n    background-color: #dd3333;\\r\\n    color: #fff;\\r\\n    text-align: center;\\r\\n    padding: 7px;\\r\\n    font-size: 20px;\\r\\n    border-top-left-radius: 35px;\\r\\n}\\r\\n\\r\\n@media (max-width:768px) {\\r\\n    .trigger_popup {\\r\\n        transform: rotate(0);\\r\\n        bottom: 0;\\r\\n        top: 90%;\\r\\n        right: unset;\\r\\n        margin: 0;\\r\\n        left: 15%;\\r\\n        font-size: 14px;\\r\\n        padding: 6px 10px;\\r\\n    }\\r\\n    .popupCloseButton {\\r\\n        top: -10px;\\r\\n        right: -2px;\\r\\n    }\\r\\n}'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "University Website Scraper\n",
    "A comprehensive web scraping solution for extracting data from university websites.\n",
    "Includes multiple scraping techniques, error handling, and data storage.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class UniversityData:\n",
    "    \"\"\"Data structure to hold university information\"\"\"\n",
    "    name: str\n",
    "    url: str\n",
    "    description: str\n",
    "    departments: List[str]\n",
    "    courses: List[Dict]\n",
    "    faculty: List[Dict]\n",
    "    news: List[Dict]\n",
    "    contact_info: Dict\n",
    "    scraped_at: str\n",
    "\n",
    "class UniversityScraper:\n",
    "    \"\"\"\n",
    "    A comprehensive university website scraper that handles:\n",
    "    - Static content with requests + BeautifulSoup\n",
    "    - Dynamic content with Selenium\n",
    "    - Rate limiting and respectful scraping\n",
    "    - Error handling and retry logic\n",
    "    - Data storage in multiple formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, delay: float = 1.0):\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # Setup Selenium WebDriver (headless)\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.use_selenium = True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Selenium setup failed: {e}. Using requests only.\")\n",
    "            self.use_selenium = False\n",
    "    \n",
    "    def get_page_content(self, url: str, use_selenium: bool = False) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        Fetch page content using either requests or Selenium\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if use_selenium and self.use_selenium:\n",
    "                self.driver.get(url)\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                html = self.driver.page_source\n",
    "                return BeautifulSoup(html, 'html.parser')\n",
    "            else:\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_university_info(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"Extract basic university information\"\"\"\n",
    "        info = {}\n",
    "        \n",
    "        # Extract university name\n",
    "        name_selectors = ['h1', '.university-name', '#university-name', 'title']\n",
    "        for selector in name_selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                info['name'] = element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Extract description/mission\n",
    "        desc_selectors = ['.description', '.mission', '.about', '#about']\n",
    "        for selector in desc_selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                info['description'] = element.get_text(strip=True)[:1000]  # Limit length\n",
    "                break\n",
    "        \n",
    "        # Extract contact information\n",
    "        contact_info = {}\n",
    "        \n",
    "        # Phone numbers\n",
    "        phone_pattern = r'[\\+]?[1-9]?[0-9]{3}[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}'\n",
    "        phones = re.findall(phone_pattern, str(soup))\n",
    "        if phones:\n",
    "            contact_info['phones'] = list(set(phones))\n",
    "        \n",
    "        # Email addresses\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, str(soup))\n",
    "        if emails:\n",
    "            contact_info['emails'] = list(set(emails))\n",
    "        \n",
    "        # Address\n",
    "        address_keywords = ['address', 'location', 'campus']\n",
    "        for keyword in address_keywords:\n",
    "            addr_elem = soup.find(text=re.compile(keyword, re.I))\n",
    "            if addr_elem:\n",
    "                parent = addr_elem.parent\n",
    "                if parent:\n",
    "                    contact_info['address'] = parent.get_text(strip=True)\n",
    "                    break\n",
    "        \n",
    "        info['contact_info'] = contact_info\n",
    "        return info\n",
    "    \n",
    "    def scrape_departments(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract department/faculty information\"\"\"\n",
    "        departments = []\n",
    "        \n",
    "        # Common selectors for departments\n",
    "        dept_selectors = [\n",
    "            '.department', '.faculty', '.school',\n",
    "            'a[href*=\"department\"]', 'a[href*=\"faculty\"]',\n",
    "            'a[href*=\"school\"]', '.nav-item a'\n",
    "        ]\n",
    "        \n",
    "        for selector in dept_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for elem in elements:\n",
    "                dept_name = elem.get_text(strip=True)\n",
    "                if dept_name and len(dept_name) > 3:  # Filter out very short names\n",
    "                    departments.append(dept_name)\n",
    "        \n",
    "        return list(set(departments))  # Remove duplicates\n",
    "    \n",
    "    def scrape_courses(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extract course information\"\"\"\n",
    "        courses = []\n",
    "        \n",
    "        # Look for course listings\n",
    "        course_selectors = [\n",
    "            '.course', '.program', '.degree',\n",
    "            'a[href*=\"course\"]', 'a[href*=\"program\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in course_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for elem in elements:\n",
    "                course_name = elem.get_text(strip=True)\n",
    "                course_link = elem.get('href') if elem.name == 'a' else None\n",
    "                \n",
    "                if course_link:\n",
    "                    course_link = urljoin(self.base_url, course_link)\n",
    "                \n",
    "                if course_name and len(course_name) > 5:\n",
    "                    courses.append({\n",
    "                        'name': course_name,\n",
    "                        'url': course_link,\n",
    "                        'description': ''  # Could be extracted from course pages\n",
    "                    })\n",
    "        \n",
    "        return courses[:50]  # Limit to first 50 courses\n",
    "    \n",
    "    def scrape_faculty(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extract faculty information\"\"\"\n",
    "        faculty = []\n",
    "        \n",
    "        # Look for faculty/staff listings\n",
    "        faculty_selectors = [\n",
    "            '.faculty-member', '.staff-member', '.professor',\n",
    "            'a[href*=\"faculty\"]', 'a[href*=\"staff\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in faculty_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for elem in elements:\n",
    "                name = elem.get_text(strip=True)\n",
    "                profile_link = elem.get('href') if elem.name == 'a' else None\n",
    "                \n",
    "                if profile_link:\n",
    "                    profile_link = urljoin(self.base_url, profile_link)\n",
    "                \n",
    "                if name and len(name) > 3:\n",
    "                    faculty.append({\n",
    "                        'name': name,\n",
    "                        'profile_url': profile_link,\n",
    "                        'department': '',  # Could be extracted from profile\n",
    "                        'title': ''        # Could be extracted from profile\n",
    "                    })\n",
    "        \n",
    "        return faculty[:30]  # Limit to first 30 faculty\n",
    "    \n",
    "    def scrape_news(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extract news/announcements\"\"\"\n",
    "        news = []\n",
    "        \n",
    "        # Look for news items\n",
    "        news_selectors = [\n",
    "            '.news-item', '.announcement', '.article',\n",
    "            'a[href*=\"news\"]', 'a[href*=\"announcement\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in news_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for elem in elements:\n",
    "                title = elem.get_text(strip=True)\n",
    "                news_link = elem.get('href') if elem.name == 'a' else None\n",
    "                \n",
    "                if news_link:\n",
    "                    news_link = urljoin(self.base_url, news_link)\n",
    "                \n",
    "                # Try to extract date\n",
    "                date_elem = elem.find(class_=re.compile(r'date|time'))\n",
    "                date = date_elem.get_text(strip=True) if date_elem else ''\n",
    "                \n",
    "                if title and len(title) > 10:\n",
    "                    news.append({\n",
    "                        'title': title,\n",
    "                        'url': news_link,\n",
    "                        'date': date,\n",
    "                        'summary': ''  # Could be extracted from news pages\n",
    "                    })\n",
    "        \n",
    "        return news[:20]  # Limit to first 20 news items\n",
    "    \n",
    "    def scrape_additional_pages(self, urls: List[str]) -> Dict:\n",
    "        \"\"\"Scrape additional pages like About, Academics, etc.\"\"\"\n",
    "        additional_data = {}\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                soup = self.get_page_content(url)\n",
    "                if soup:\n",
    "                    # Extract text content\n",
    "                    text_content = soup.get_text()\n",
    "                    # Clean up text\n",
    "                    text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
    "                    \n",
    "                    # Store first 2000 characters\n",
    "                    page_name = urlparse(url).path.split('/')[-1] or 'main'\n",
    "                    additional_data[page_name] = text_content[:2000]\n",
    "                \n",
    "                time.sleep(self.delay)  # Rate limiting\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "        return additional_data\n",
    "    \n",
    "    def discover_urls(self, soup: BeautifulSoup, max_urls: int = 10) -> List[str]:\n",
    "        \"\"\"Discover important URLs to scrape\"\"\"\n",
    "        urls = []\n",
    "        \n",
    "        # Keywords for important pages\n",
    "        important_keywords = [\n",
    "            'about', 'academics', 'admissions', 'faculty',\n",
    "            'courses', 'programs', 'departments', 'news'\n",
    "        ]\n",
    "        \n",
    "        # Find all links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text(strip=True).lower()\n",
    "            \n",
    "            # Check if URL contains important keywords\n",
    "            for keyword in important_keywords:\n",
    "                if keyword in href.lower() or keyword in text:\n",
    "                    full_url = urljoin(self.base_url, href)\n",
    "                    if full_url not in urls and len(urls) < max_urls:\n",
    "                        urls.append(full_url)\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    def scrape_university(self) -> UniversityData:\n",
    "        \"\"\"Main scraping function\"\"\"\n",
    "        logger.info(f\"Starting to scrape {self.base_url}\")\n",
    "        \n",
    "        # Get main page\n",
    "        main_soup = self.get_page_content(self.base_url)\n",
    "        if not main_soup:\n",
    "            raise Exception(f\"Could not fetch main page: {self.base_url}\")\n",
    "        \n",
    "        # Extract basic info\n",
    "        basic_info = self.extract_university_info(main_soup)\n",
    "        \n",
    "        # Extract structured data\n",
    "        departments = self.scrape_departments(main_soup)\n",
    "        courses = self.scrape_courses(main_soup)\n",
    "        faculty = self.scrape_faculty(main_soup)\n",
    "        news = self.scrape_news(main_soup)\n",
    "        \n",
    "        # Discover and scrape additional pages\n",
    "        additional_urls = self.discover_urls(main_soup)\n",
    "        additional_data = self.scrape_additional_pages(additional_urls)\n",
    "        \n",
    "        # Create UniversityData object\n",
    "        university_data = UniversityData(\n",
    "            name=basic_info.get('name', 'Unknown University'),\n",
    "            url=self.base_url,\n",
    "            description=basic_info.get('description', ''),\n",
    "            departments=departments,\n",
    "            courses=courses,\n",
    "            faculty=faculty,\n",
    "            news=news,\n",
    "            contact_info=basic_info.get('contact_info', {}),\n",
    "            scraped_at=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Scraping completed. Found {len(departments)} departments, {len(courses)} courses, {len(faculty)} faculty, {len(news)} news items\")\n",
    "        \n",
    "        return university_data\n",
    "    \n",
    "    def save_data(self, data: UniversityData, output_dir: str = \"scraped_data\"):\n",
    "        \"\"\"Save scraped data in multiple formats\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_path = os.path.join(output_dir, f\"{data.name.replace(' ', '_')}.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data.__dict__, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save courses as CSV\n",
    "        if data.courses:\n",
    "            courses_df = pd.DataFrame(data.courses)\n",
    "            courses_path = os.path.join(output_dir, f\"{data.name.replace(' ', '_')}_courses.csv\")\n",
    "            courses_df.to_csv(courses_path, index=False)\n",
    "        \n",
    "        # Save faculty as CSV\n",
    "        if data.faculty:\n",
    "            faculty_df = pd.DataFrame(data.faculty)\n",
    "            faculty_path = os.path.join(output_dir, f\"{data.name.replace(' ', '_')}_faculty.csv\")\n",
    "            faculty_df.to_csv(faculty_path, index=False)\n",
    "        \n",
    "        # Save news as CSV\n",
    "        if data.news:\n",
    "            news_df = pd.DataFrame(data.news)\n",
    "            news_path = os.path.join(output_dir, f\"{data.name.replace(' ', '_')}_news.csv\")\n",
    "            news_df.to_csv(news_path, index=False)\n",
    "        \n",
    "        logger.info(f\"Data saved to {output_dir}\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup Selenium driver\"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Example usage and demonstration\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the UniversityScraper\n",
    "    \"\"\"\n",
    "    # Example universities to scrape\n",
    "    universities = [\n",
    "        \"https://www.kluniversity.in\"\n",
    "    ]\n",
    "    \n",
    "    # You can also use a specific university\n",
    "    # university_url = \"https://www.your-university.edu\"\n",
    "    \n",
    "    for university_url in universities[:1]:  # Scrape only first one for demo\n",
    "        try:\n",
    "            # Create scraper instance\n",
    "            scraper = UniversityScraper(university_url, delay=2.0)\n",
    "            \n",
    "            # Scrape the university\n",
    "            university_data = scraper.scrape_university()\n",
    "            \n",
    "            # Save the data\n",
    "            scraper.save_data(university_data)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nðŸ« University: {university_data.name}\")\n",
    "            print(f\"ðŸ“ Description: {university_data.description[:200]}...\")\n",
    "            print(f\"ðŸ¢ Departments: {len(university_data.departments)}\")\n",
    "            print(f\"ðŸ“š Courses: {len(university_data.courses)}\")\n",
    "            print(f\"ðŸ‘¨â€ðŸ« Faculty: {len(university_data.faculty)}\")\n",
    "            print(f\"ðŸ“° News: {len(university_data.news)}\")\n",
    "            print(f\"ðŸ“ž Contact: {university_data.contact_info}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {university_url}: {e}\")\n",
    "\n",
    "# Advanced scraping utilities\n",
    "class AdvancedUniversityScraper(UniversityScraper):\n",
    "    \"\"\"\n",
    "    Extended scraper with additional features:\n",
    "    - Handle JavaScript-heavy sites\n",
    "    - Extract course syllabi\n",
    "    - Scrape research papers\n",
    "    - Handle authentication\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_course_details(self, course_url: str) -> Dict:\n",
    "        \"\"\"Scrape detailed course information\"\"\"\n",
    "        soup = self.get_page_content(course_url, use_selenium=True)\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        course_details = {}\n",
    "        \n",
    "        # Extract course code\n",
    "        code_pattern = r'[A-Z]{2,4}[\\s-]?\\d{3,4}'\n",
    "        code_match = re.search(code_pattern, soup.get_text())\n",
    "        if code_match:\n",
    "            course_details['code'] = code_match.group()\n",
    "        \n",
    "        # Extract credits\n",
    "        credit_pattern = r'(\\d+)[\\s-]?credit'\n",
    "        credit_match = re.search(credit_pattern, soup.get_text(), re.I)\n",
    "        if credit_match:\n",
    "            course_details['credits'] = int(credit_match.group(1))\n",
    "        \n",
    "        # Extract prerequisites\n",
    "        prereq_section = soup.find(text=re.compile('prerequisite', re.I))\n",
    "        if prereq_section:\n",
    "            prereq_text = prereq_section.parent.get_text(strip=True)\n",
    "            course_details['prerequisites'] = prereq_text\n",
    "        \n",
    "        return course_details\n",
    "    \n",
    "    def scrape_with_authentication(self, login_url: str, username: str, password: str):\n",
    "        \"\"\"Handle sites that require authentication\"\"\"\n",
    "        if not self.use_selenium:\n",
    "            logger.error(\"Authentication scraping requires Selenium\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(login_url)\n",
    "            \n",
    "            # Find and fill login form\n",
    "            username_field = self.driver.find_element(By.NAME, \"username\")\n",
    "            password_field = self.driver.find_element(By.NAME, \"password\")\n",
    "            \n",
    "            username_field.send_keys(username)\n",
    "            password_field.send_keys(password)\n",
    "            \n",
    "            # Submit form\n",
    "            login_button = self.driver.find_element(By.TYPE, \"submit\")\n",
    "            login_button.click()\n",
    "            \n",
    "            # Wait for login to complete\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Authentication successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Authentication failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a59bc4-ee87-48cb-a2eb-9dbd2db848cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
